# dishdecoder
üçΩÔ∏è

### RAPPORT SUR LE PROJET DISHDECODER

PLAN :

I. Introduction

    1.1 Contexte
    1.2 Objectifs
    1.3 Approche de la solution
    1.4 Technologies utilis√©es
    1.5 Sch√©ma de l‚Äôapplication

II. ETAT DE L‚ÄôART

III. DISHDECODER

    3.1 Le mod√®le d'OCR
        3.1.1 Le choix du mod√®le
        3.1.2 Les donn√©es n√©cessaires √† l‚Äôentra√Ænement du mod√®le
        3.1.3 La cr√©ation du dataset
        3.1.4 Le pr√©traitement des donn√©es
        3.1.5 Le stockage du dataset
        3.1.6 L‚Äôentra√Ænement du mod√®le
        3.1.7 Configuration des hyperparam√®tres du mod√®le
        3.1.8 Comparaison des mod√®les et choix du meilleur
        3.1.9 Le stockage du mod√®le

    3.2 Les mod√®les de traduction
        3.2.1 Le choix des mod√®les
        3.2.2 Les donn√©es n√©cessaires √† l‚Äôentra√Ænement des mod√®les
        3.2.3 La cr√©ation du dataset
        3.2.4 Le pr√©traitement des donn√©es
        3.2.5 Le stockage du dataset
        3.2.6 L‚Äôentra√Ænement des mod√®les
        3.2.7 Configuration des hyperparam√®tres des mod√®les
        3.2.8 Comparaison des mod√®les et choix des meilleurs
        3.2.9 Le stockage des mod√®les

    3.3 L'utilisation d'Open AI
        3.3.1 Pourquoi le choix d'Open AI ?
        3.3.2 L'API d'Open AI
        3.3.3 Le mod√®le utilis√©
        3.3.4 Le prompt

    3.4 Le back-end
        3.4.1 L‚ÄôAPI
        3.4.2 La sch√©matisation de l‚ÄôAPI
        3.4.3 La base de donn√©es relationnelle
        3.4.4 Le stockage des images des utilisateurs

    3.5 Le front-end
        3.5.1 L'utilisation de Streamlit
        3.5.2 La sch√©matisation du front-end
        3.5.3 Les fonctionnalit√©s de l'application
        3.5.4 Le parcours utilisateur
        3.5.5 Le parcours administrateur
    
    3.6 L'organisation du projet
        3.6.1 Le Trello (GANT ?)
        3.6.2 Le Kanban
        3.6.2 Le GitHub

IV. CONCLUSION

    4.1 La r√©alisation du projet
    4.2 Les difficult√©s rencontr√©es
    4.3 Les am√©liorations possibles
    4.4 Les perspectives d'√©volution
    

# I. Introduction

## 1.1. Contexte

Depuis plusieurs mois maintenant, l'Intelligence Artificielle est au coeur de l'actualit√©. Tant√¥t adul√©e, par certains qui voient en elle un moyen de r√©soudre les probl√®mes de l'humanit√©, tant√¥t d√©cri√©e, par d'autres qui la voient comme une menace pour l'emploi et la survie de l'esp√®ce, l'Intelligence Artificielle est un sujet qui fait d√©bat. 
C'est dans ce contexte que je me suis mise en qu√™te d'un projet √† r√©aliser o√π je pourrais m√™ler √† la fois mes connaissances en IA et un sujet qui est tr√®s terre √† terre : la nourriture. Les premi√®res id√©es qui me sont venues concernaient la lecture des ingr√©dients pr√©sents sur les emballages alimentaires et la d√©tection de substances dangereuses dans certains plats pr√©par√©s, cependant de nombreuses applications exitent d√©j√† sur le march√© et je ne voyais pas comment je pourrais apporter une plus-value √† ce qui existe d√©j√†.
C'est au d√©tour d'une conversation avec mon p√®re que l'id√©e de Dishdecoder m'est venue. En effet, celui-ci me demandait si je connaissais une application o√π lorsqu'on lui envoie une photo avec du texte celle-ci est capable de renvoyer une traduction de ce texte dans la langue souhait√©e. Bien sur, il existe d√©j√† des applications qui extraient du texte pr√©sent sur une image, des applications faisant de la traduction, et m√™me des applications faisant les deux comme par exemple TextGrabber, mais je me suis dit que je pourrais essayer de faire quelque chose de similaire, sp√©cialis√© dans la cuisine et plus pr√©cis√©ment dans la lecture et la traduction de recettes de cuisine. C'est ainsi que Dishdecoder est n√©.

## 1.2. Objectifs

L'objectif de Dishdecoder est d'√™tre une application permettant √† un utilisateur de prendre en photo une recette de cuisine et d'obtenir une traduction de celle-ci dans la langue de son choix. L'application doit √™tre capable de d√©tecter le texte pr√©sent sur l'image, de le reconnaitre et de le traduire. Dans un premier temps, l'application devra √™tre capable de fournir des traductions de recettes de cuisine en anglais et en fran√ßais. L'application devra √©galement proposer √† l'utilisateur de sauvegarder les recettes traduites dans un espace personnel. Enfin, l'application devra permettre √† l'administrateur de voir les traductions r√©alis√©es afin de pouvoir r√©colter des donn√©es pour am√©liorer les mod√®les de traduction.

## 1.3 Approche de la solution

Pour r√©aliser ce projet, plusieurs composants allaient √™tre n√©cessaires:
- Les mod√®les d'intelligence artificielle : celui d'extraction de texte √† partir d'image et ceux de traduction
- Les bases de donn√©es : celles contenant les datasets pour entrainer les mod√®les et celle contenant les donn√©es de l'application
- La mise en place du back-end de l'application : l√† o√π les mod√®les communiqueront avec l'API
- La r√©alisation du front-end de l'application : l√† o√π l'utilisateur pourra interagir avec l'application qui elle m√™me inter√©agira avec l'API

## 1.4 Technologies utilis√©es

Pour r√©aliser ce projet, j'ai utilis√© les technologies suivantes :
- Python : comme langage de programmation
- Pytorch : pour la r√©alisation des mod√®les d'intelligence artificielle
- FastAPI : pour la r√©alisation de l'API
- SQLModel : pour la r√©alisation de la base de donn√©es relationnelle
- Streamlit : pour la r√©alisation du front-end de l'application
- HuggingFace Hub : pour le stockage des mod√®les d'intelligence artificielle et des datasets
- Cloudinary : pour le stockage d'images
- PadddleOCR : pour la r√©alisation du mod√®le d'extraction de texte √† partir d'image
- HuggingFace : pour la r√©alisation du mod√®le de traduction
- OpenAI : pour l'utilisation de leur API 
- Playground AI : pour la r√©alisation de l'√©g√©rie de l'application


## 1.5 Sch√©ma de l'application

INSERER ICI LE SCH√âMA DE L'APPLICATION

# II. ETAT DE L'ART

INSERER ICI L'ETAT DE L'ART

# III. DISHDECODER

## 3.1 Le mod√®le d'OCR

### 3.1.1 Le choix du mod√®le

Comme nous l'avons vu pr√©c√©demment, il existe √† l'heure actuelle de tr√®s nombreux mod√®les pour r√©aliser de l'OCR (Optical Character Recognition).

Lors de mon alternance j'ai √©t√© en charge de la r√©alisation d'un mod√®le d'OCR pour une application de lecture de documents. J'ai donc eu l'occasion de tester plusieurs mod√®les d'OCR et de voir leurs avantages et leurs inconv√©nients. Parmi les mod√®les que j'ai test√©, il y avait Tesseract, EasyOCR, TROCR et PaddleOCR. 

Tesseract, qui est sans doute le plus connu, a √©t√© d√©velopp√© √† l'origine par Hewlett-Packard Labs et d√©sormais maintenu par Google. Tesseract est un mod√®le open source et est de base fait pour √™tre utilis√© en language C++ mais il existe des wrappers pour l'utiliser en Python, comme par exemple Pytesseract. Cependant, apr√®s test, Tesseract n'est pas le mod√®le le plus performant, notamment sur les images avec du bruit. De plus, il n'est pas toujours simple √† configurer selon l'utilisation que l'on souhaite en faire et aussi son impl√©mentation en Python peut se r√©veler tr√®s compliqu√©e en raison d'incompatibilit√©s avec d'autres librairies. C'est pourquoi, lors de mon alternance, j'ai d√©cid√© de ne pas utiliser Tesseract.

EasyOCR est quant √† lui connu pour √™tre une librairie open-source, d√©velop√©e par JaidedAi, qui prend en charge plus de 70 langues et qui est compatible avec de nombreux langages de programmation dont Python, C#, Java. En plus de cela, EasyOCR est tr√®s rapide. Par ailleurs, comme son nom l‚Äôindique EasyOCR est facile √† utiliser. Il suffit de trois lignes, dont l'importation, pour qu‚Äôil lise une image. Cependant, j'avais remarqu√© que les sorties de celui-ci n‚Äô√©taient pas toujours exactes. Que ce soit pour la lecture des chiffres pr√©sents sur les documents ou la lecture des diff√©rentes zones de texte, de nombreuses erreurs √©taient r√©alis√©es √† chaque lecture : confusion entre deux caract√®res, oubli d‚Äôespace, ajout de caract√®res etc. Malgr√© divers r√©glages dans les param√®tres, les r√©sultats restaient toujours brouillons.

TROCR (Tokenization, Recognizing, and OCR) est √©galement une librairie open-source, datant de 2017, d√©velopp√©e par Microsoft et impl√©ment√©e
dans Transformers, la librairie open-source d‚ÄôHugging Face. TROCR combine les √©tapes d‚ÄôOCR pour extraire le texte des images, et de tokenisation pour convertir le texte en une s√©quence de tokens utilisable par des mod√®les de traitement du langage naturel. Contrairement √† EasyOCR, le traitement des images via TROCR est beaucoup plus long. Malgr√© cela j'ai quand m√™me test√© TROCR, avec le mod√®le base-printed qui est cens√© √™tre le mod√®le le plus adapt√© √† la lecture de caract√®res d‚Äôimprimerie. En effet, le mod√®le base-printed a √©t√© fine-tun√© sur le dataset SROIE qui est constitu√© d‚Äôun millier de tickets de caisse. Apr√®s de nombreux tests, les r√©sultats obtenus avec TROCR √©taient g√©n√©ralement meilleurs que ceux obtenus avec EasyOCR. Toutefois, TROCR donnait exclusivement des sorties en majuscules, la ponctuation √©tait approximative, les espaces pas toujours respect√©s et surtout, le temps d‚Äôinf√©rence du mod√®le √©tait beaucoup trop important. TROCR doit √™tre r√©serv√© √† un usage sur GPU, ce qui n'√©tait pas possible lors de mon altenance et qui ne l'est toujours pas aujourd'hui avec Dishdecoder.

PaddleOCR est, comme pour les trois OCR pr√©c√©dents, une librairie open-source. Celle-ci est d√©velopp√©e par PaddlePaddle (PArallel Distributed Deep LEarning), une entreprise chinoise. Elle utilise des algorithmes de pointe pour d√©tecter, localiser et reconna√Ætre des caract√®res dans des images, et est disponible dans de
nombreuses langues. L‚Äôutilisation de PaddleOCR ressemble √† celle d‚ÄôEasyOCR par sa facilit√©. L√† aussi, la librairie est compatible avec plusieurs langages de programmation tels que Python, C++ et Java, ce qui facilite son int√©gration dans diff√©rents projets.
L‚Äôarchitecture de deep-learning utilis√©e derri√®re PaddleOCR est un CRNN : Convolutional Recurrent Neural Network. Le CRNN est un r√©seau de neurones profonds qui combine √† la fois un CNN (Convolutional Neural Network) et un RNN (Recurrent Neural Network). 
Dans un CRNN, la couche de convolution, CNN, est utilis√©e pour extraire des caract√©ristiques pertinentes des images. Ensuite, la couche RNN est utilis√©e pour analyser et traiter ces caract√©ristiques dans un contexte de s√©quence. Les sorties de la couche de convolution sont
transform√©es en s√©quences d'entr√©e pour le RNN, qui utilise sa m√©moire interne pour prendre en compte les caract√©ristiques pr√©c√©dentes tout en analysant la s√©quence actuelle. Cela permet au CRNN de mieux comprendre la structure de l'image et d'extraire des informations textuelles
pertinentes.
Concernant PaddlePaddle, il s‚Äôagit de la premi√®re plateforme open-source ind√©pendante de deep learning en Chine. Elle est d√©velopp√©e par l‚Äôentreprise Baidu. PaddlePaddle est plus ou moins l‚Äô√©quivalent chinois de TensorFlow. La plateforme PaddlePaddle est r√©put√©e pour sa rapidit√© d‚Äôinf√©rence, sa capacit√© √† traiter des volumes de donn√©es massifs et son large √©cosyst√®me de modules et d'outils int√©gr√©s. La plateforme est largement utilis√©e dans diff√©rents domaines, notamment l'industrie, le transport, l‚Äôagriculture et les services publics. 
D√®s la premi√®re utilisation de PaddleOCR, avec le mod√®le de base en langue fran√ßaise et latine, j‚Äôai  pu observer que les sorties donn√©es √©taient bien meilleures que celles d‚ÄôEasyOCR. Certains points √©taient malgr√© tout perfectibles : les espaces entre les caract√®res n‚Äô√©taient souvent pas retranscrits et il y avait parfois de la confusion entre certains caract√®res. Cependant, il √©tait mis en avant sur le GitHub de PaddleOCR qu'avec du finetuning, les r√©sultats pouvaient √™tre nettement am√©lior√©s. 

Voici un exemple de test que j'ai r√©alis√© lors de mon alternance sur EasyOCR, TROCR et PaddleOCR :

INSERER ICI LES IMAGES MR OU MME BERTIN


Comme nous pouvons l'observer, les r√©sultats obtenus par TROCR et PaddleOCR sont bien concluants que ceux obtenus par EasyOCR. 

C'est donc pour ces diverses raisons que lors de mon alternance j'ai d√©cid√© de finetuner PaddleOCR. Les bons r√©sultats obtenus font que mon mod√®le finetun√© est d√©sormais le mod√®le utilis√© par mon entreprise d'alternance lorsqu'il s'agit de faire de l'OCR.

Ne pouvant r√©utiliser le mod√®le et le dataset d'entra√Ænement de mon alternance pour Dishdecoder, j'ai choisi de reconstituer un nouveau dataset d'entra√Ænement et de finetuner un mod√®le de PaddleOCR sur ce nouveau dataset.

### 3.1.2 Les donn√©es n√©cessaires √† l'entra√Ænement du mod√®le

Avant de savoir quelles donn√©es sont n√©cessaires pour finetuner un mod√®le de PaddleOCR, il est important de comprendre comment fonctionne PaddleOCR.
En effet, il n'existe pas qu'un seul mod√®le PaddleOCR qui sait g√©rer toutes les langues. Il existe un mod√®le par langue. Par exemple, il existe un mod√®le pour la langue fran√ßaise, un mod√®le pour la langue anglaise, un mod√®le pour la langue chinoise etc. Il y a √©galement plusieurs versions, il y a par exemple les mod√®le PP-OCRv2 de l'ancienne g√©n√©ration et les PP-OCRv3 qui sont les plus r√©cents et les plus performants. A vitesse √©gale, selon les langues, les mod√®les PP-OCRv3 sont entre 5% et 11% plus pr√©cis (accuracy) que les mod√®les PP-OCRv2.
Par ailleurs, PaddleOCR propose d‚Äôutiliser trois types de mod√®les diff√©rents qui peuvent √™tre combin√©s lors de la lecture d‚Äôune image. Il y a un mod√®le de reconnaissance de caract√®res (recognition model aussi appel√© rec), un mod√®le de d√©tection de texte (detection model aussi appel√© det) et un mod√®le de classification d‚Äôangle du texte (classification model aussi appel√© cls). Dans mon cas d‚Äôusage, je n‚Äôavais pas besoin de finetuner mon mod√®le pour qu‚Äôil soit plus performant dans la d√©tection de texte ou dans la reconnaissance de l‚Äôorientation du texte. Je souhaitais uniquement avoir un mod√®le plus performant sur la reconnaissance de caract√®res, c‚Äô√©tait donc un recognition model que j‚Äôallais finetuner. Par ailleurs, plut√¥t que de finetuner le mod√®le de reconnaissance fran√ßais, french_mobile_v2.0_rec, j‚Äôai choisi de finetuner le mod√®le de reconnaissance latin appel√©
latin_PP-OCRv3_rec. En effet, le mod√®le latin fait partie des mod√®les de 3√®me g√©n√©ration, ce qui implique de meilleures performances.

Il est recommand√©, sur le GitHub de PaddleOCR, d'utiliser plus de 5000 images lors du finetuning d'un mod√®le de reconnaissance. 

D'experience, je sais √©galement que plus le dataset comprend des images diverses et vari√©es et plus le mod√®le finetun√© est performant. Il √©tait donc n√©cessaires que je receuille des images avec diff√©rentes tailles, diff√©rentes polices de caract√®res, diff√©rents types de fonds, diff√©rentes couleurs, diff√©rentes longueur de texte, des majuscules, des minuscules, des chiffres, des caract√®res sp√©ciaux etc.

Souhaitant √©galement que mon OCR soit performant sur du fran√ßais comme sur de l'anglais, j'ai d√©cid√© de r√©colter des images en fran√ßais et en anglais.

Pour finetuner un mod√®le de reconnaissance PaddleOCR, le dataset doit √™tre constitu√© de la mani√®re suivante : il doit y avoit des images et un fichier texte. Le fichier texte doit contenir deux √©l√©ments pour chaque image: le chemin de sa localisation avec le nom donn√© √† l'image et le texte pr√©sent sur l'image. Le chemin de l'image et le texte pr√©sent sur l'image doivent √™tre s√©par√©s par une tabulation. 

### 3.1.3 La cr√©ation du dataset

N'ayant trouv√© aucun dataset pertinent r√©pondant √† mes besoins, j'ai d√©cid√©, comme lors de mon alternance, de cr√©er mon propre dataset.
Pour cela, aucun outil de labellisation n'est n√©cessaire. 
En effet, le seul outil dont j'avais besoin √©tait un outil de capture d'√©cran. J'ai donc utilis√© l'outil de capture d'√©cran int√©gr√© √† mon ordinateur, celui de Ubuntu.
Afin de constitu√© mon dataset, j'ai √©t√© sur de nombreux sites internet dont : 
- Amazon.com o√π j'ai recherch√© diff√©rents ouvrages en fran√ßais et en anglais. J'y ai captur√© des images de titres de livres, de quatri√®me de couverture, de table des mati√®res...
- Brest.fr o√π j'ai pu trouver certains extraits de magazines, des infographies, des affiches, des publicit√©s...
- Des sites internet d'archive o√π sont scann√©s d'anciens num√©ros de journaux actuels comme Le Monde ou Le Canard Encha√Æn√©, o√π j'ai pu capturer des images de titres d'articles etc.
- Des sites internet sp√©cialis√©s dans les journaux et annuaires anciens pour trouver des polices d'√©criture plus rares.
- Des sites internet de restaurants dont Restaurantguru.com o√π j'ai pu capturer des images √† partir des diff√©rents menus.

J'ai r√©colt√© en tout 1285 images. Pour chaque image, j'ai recens√© son chemin avec son nom ainsi que le texte pr√©sent dans cette m√™me image, le tout consign√© dans un fichier texte.

### 3.1.4 Le pr√©traitement des donn√©es

Parmi les images r√©cup√©r√©es, j'en ai s√©lectionn√© 565 pour l'entra√Ænement et 720 pour la validation.
Afin de rendre mon dataset plus robuste, j'ai fait de la data augmentation sur mon dataset d'entrainement. 
Pour cela, j'ai adapt√© le code du projet Straug pr√©sent sur le GitHub de Roatenzia √† ce dont j‚Äôavais besoin.
J‚Äôai choisi d‚Äôappliquer √† mes images les effets suivants : GaussianNoise, SpeckleNoise, ImpulseNoise, MotionBlur, DefocusBlur, Shadow, Rain. 

INSRER IMAGE EXEMPLE

Une fois les nouvelles images g√©n√©r√©es, 17 nouvelles images en sortie pour 1 image en entr√©e, soit 9605 nouvelles images, j‚Äôai pu les labelliser dans mon fichier texte d‚Äôentra√Ænement et les d√©placer dans mon dossier comprenant toutes les images.
Ainsi le dataset final pour ce finetuning comprenait 10170 images pour l‚Äôentra√Ænement et 720 images pour la validation.
L'ensemble du dataset a √©t√© mis dans un dossier train_data. Dans train_data il y a un dossier data o√π sont stock√©es toutes les images du dataset et deux fichiers .txt : dataset_IMG_train_REDO.txt et dataset_IMG_val_REDO.txt . Comme leurs noms le laisse supposer, le fichier dataset_IMG_train_REDO.txt contient les chemins ainsi que le texte contenu sur les images d'entra√Ænement et le fichier dataset_IMG_val_REDO.txt contient les chemins ainsi que le texte contenu sur les images de validation.

### 3.1.5 Le stockage du dataset

J'aurais souhait√© stocker mon dataset sur le Hub de Hugging Face, mais cela n'a pas pu √™tre fait car un repository sur HuggingFace ne peut pas contenir plus de 10000 √©l√©ments. J'ai donc stock√© mon dataset dans un dossier sur mon ordinateur. 

### 3.1.6 L'entrainement du mod√®le

La premi√®re √©tape avant de lancer l'entra√Ænement, fut de cloner le repository GitHub de PaddleOCR sur mon ordinateur.
```$ git clone https://github.com/PaddlePaddle/PaddleOCR.git```
Une fois le projet clon√©, il fallait, √† l‚Äôint√©rieur du dossier principal, cr√©er un dossier pretrain_models.
En effet, lorsque l‚Äôon finetune un mod√®le, il nous faut obligatoirement un mod√®le de base, mod√®le auquel on va apporter des modifications avec le finetuning. Pour les raisons expliqu√©es plus haut, j‚Äôavais choisi le mod√®le latin_PP-OCRv3_rec. Il fallait alors t√©l√©charger ce mod√®le depuis le GitHub de PaddleOCR et l‚Äôextraire dans le dossier pretrain_models.
Apr√®s cela, √† la racine du dossier principal PaddleOCR, au m√™me niveau que pour le dossier pretrain_models, il fallait copier le dossier train_data comprenant le dataset.
L‚Äô√©tape suivante √©tait de retourner dans le dossier principal PaddleOCR, puis d‚Äôaller dans le dossier configs, puis rec, puis PP-OCRv3, puis multi_language, puis de faire une copie du fichier latin_PPOCRv3_rec.yml en le renommant. C‚Äôest dans ce fichier de configuration qu‚Äôinterviennent les
modifications de chemins afin de permettre √† PaddleOCR d‚Äôutiliser le dataset contenu dans train_data lors de l‚Äôentra√Ænement. Ce fichier de configuration permet √©galement de sp√©cifier bon nombre de param√®tres : le chemin du dictionnaire √† utiliser, la prise en compte des espaces, mais
√©galement le chemin du mod√®le pr√©-entra√Æn√© √† utiliser etc.
Une fois toutes ces √©tapes r√©alis√©es, j'ai d√©cid√© de cr√©er un nouvel environnement conda d√©di√© √† l'entrainement de mon mod√®le. Pour cela, j'ai utilis√© la commande suivante : 
```$ conda create --name paddle-training python=3.10```
J'ai ensuite activ√© cet environnement avec la commande suivante : 
```$ conda activate paddle-training```
Apr√®s cela, afin d'utiliser la version GPU pour l'entra√Ænement du mod√®le, j'ai fait la commande suivante qui est recommand√©e sur le site de Paddlepaddle: 
```python -m pip install paddlepaddle-gpu==2.4.2.post117 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html```
Enfin j'ai install√©, en prenant bien soin d'√™tre √† ce moment-l√† dans le dossier PaddleOCR, les d√©pendances n√©cessaires √† l'entra√Ænement du mod√®le avec la commande suivante : 
```$ pip install -r requirements.txt```
Une fois toutes ces √©tapes r√©alis√©es, tout en √©tant toujours bien √† la racine du dossier PaddleOCR, j'ai pu lancer l'entra√Ænement de mon mod√®le avec la commande suivante : 
```$ python3 -m paddle.distributed.launch --gpus '0'  tools/train.py -c configs/rec/PP-OCRv3/multi_language/latin_PP-OCRv3_rec_4_finetuning.yml```

L'entra√Ænement, de 200 epochs, a √©t√© lanc√© √† 12h49 et s'est termin√© √† 23h18, soit un peu plus de 10h d'entra√Ænement.
Comme sp√©cifi√© dans le fichier de configuration, les poids du mod√®le finetun√© ayant la meilleure pr√©cision (accuracy) ainsi que ses diff√©rents fichiers (fichier de configuartaion, fichier qui repertorie tous les logs d'entra√Ænement, poids du mod√®le actualis√©s √† chaque epoch...) ont √©t√© stock√©s dans le dossier ./output/v3_latin_mobile_FINETUNING_1

Vis-√†-vis de la m√©thode d'entra√Ænement, Le finetuning est souvent consid√©r√© comme une forme d'entra√Ænement supervis√© car il n√©cessite des annotations √©tiquet√©es pour guider le processus d'apprentissage. Cependant, on peut √©galement le consid√©rer comme une approche semi-supervis√©e, car le mod√®le de base pr√©-entra√Æn√© apporte une connaissance pr√©alable supervis√©e qui est ensuite am√©lior√©e et affin√©e gr√¢ce aux donn√©es suppl√©mentaires annot√©es que nous lui fournissons. 

### 3.1.7 Configuration des hyperparam√®tres du mod√®le

Lors de ce premier finetuning, j'ai choisi de modifier les hyperparam√®tres suivants :
- le batch size, que j'ai mis √† 16
- le nombre d'epochs, que j'ai mis √† 200

Etant donn√© qu'apr√®s ce premier entra√Ænement j'ai eu l'occasion d'avoir acc√®s √† un GPU plus puissant, j'ai d√©cid√© de refaire un finetuning avec cette fois-ci un batch size de 128 et un nombre d'epochs de 200.
Ensuite, j'ai d√©cid√© de faire un troisi√®me finetuning avec un batch size de 64 et un nombre d'epochs de 200.
Puis, j'ai d√©cid√© de faire un quatri√®me finetuning avec un batch size de 64 et un nombre d'epochs de 750.
Enfin, j'ai d√©cid√© de faire un cinqui√®me finetuning avec un batch size de 32 et un nombre d'epochs de 200.

### 3.1.8 Comparaison des mod√®les et choix du meilleur 

N'ayant pas utilis√© d'outil de monitoring pour les entra√Ænements de mon mod√®le, je n'ai pas pu avoir acc√®s √† des graphiques repr√©sentant l'√©volution de l'accuracy au cours des entra√Ænements. Cependant, j'ai pu avoir acc√®s √† ces informations dans le fichier de log de chaque entra√Ænement. 
Par ailleurs, √† la fin de chaque entra√Ænement, PaddleOCR indique qu'elle fut l'epoch o√π a √©t√© calcul√©e la meilleure accuracy sur le dataset de validation. Pour PaddleOCR l'accuracy est la m√©trique utilis√©e pour √©valuer la performance du mod√®le.

Voici les r√©sultats obtenus pour chaque entra√Ænement :

INSERER ICI LES RESULTATS OBTENUS POUR CHAQUE ENTRAINEMENT

Comme nous pouvons le constater, le mod√®le ayant la meilleure accuracy est le premier √† avoir √©t√© entra√Æn√©. C'est donc celui-ci que nous allons utiliser pour la suite de ce projet.

Cependant, le mod√®le obtenu apr√®s l'entra√Ænement n'est pas utilisable en l'√©tat. Apr√®s entra√Ænement tous les mod√®les PaddleOCR sont compos√©s de trois fichiers : un fichier en .pdopt, un fichier en .pdparams et un fichier en .states. Dans notre cas ces trois fichiers sont stock√©s dans le dossier ./output/v3_latin_mobile_FINETUNING_1/best_accuracy. Afin de pouvoir utiliser ce mod√®le, il a donc fallu le convertir en un mod√®le utilisable par PaddleOCR.

 Pour cela, j'ai √©xecut√© la commande suivante : 
```$ python3 tools/export_model.py -c configs/rec/PP-OCRv3/multi_language/latin_PP-OCRv3_rec_4_finetuning.yml -o Global.pretrained_model=./output/v3_latin_mobile_FINETUNING_1/best_accuracy Global.save_inference_dir=./inference```

Une fois cette commande ex√©cut√©e, le mod√®le fut converti et stock√© dans le dossier inference. 
Le mod√®le est alors compos√© de trois nouveaux fichiers : inference.pdiparams, qui est le fichier de param√®tres du mod√®le, inference.pdiparams.info, qui est le fichier d'informations du mod√®le et inference.pdmodel, qui est le fichier de programme du mod√®le en lui-m√™me.
Afin de facilit√© l'utilisation du nouveau mod√®le, je cr√©e un dossier ocr_model et j'ai ajout√© dans ce dossier les trois fichiers d'inf√©rence convertis, le fichier de configuration du mod√®le en .yml ainsi que le dictionnaire utilis√© en .txt.

INSERER ICI UNE IMAGE DU DOSSIER OCR_MODEL

### 3.1.9 Le stockage du mod√®le

Afin de pouvoir utiliser ce mod√®le dans ce projet et dans d'autres √† venir, j'ai d√©cid√© de le stocker sur le Hub Hugging Face. Pour cela, j'ai cr√©√© un nouveau repository sur mon compte Hugging Face et j'ai ajout√© les fichiers du mod√®le dans ce repository.
Le mod√®le est donc maintenant disponible √† l'adresse suivante : https://huggingface.co/PaulineSanchez/PaddleOCR_ft

INSERER ICI UNE IMAGE DU HUB HUGGING FACE AVEC LE MODELE DANS LE REPOSITORY

# 3.2 Les mod√®les de traduction

### 3.2.1 Le choix des mod√®les

Comme pour l'OCR, une multitude de mod√®les de traduction existe. Depuis quelques ann√©es, les entreprises utilisent de plus en plus les mod√®les de traduction neuronaux. Ces mod√®les sont des mod√®les de traduction automatique qui utilisent des r√©seaux de neurones artificiels. Ils sont capables de traduire des textes d'une langue √† une autre en s'appuyant d'avantage sur le contexte. Parmi ces architectures, on retrouve notamment les RNN et les LSTM.
Les r√©seaux de neurones r√©currents (RNN) sont des mod√®les con√ßus pour traiter des s√©quences de texte en entr√©e et/ou g√©n√©rer des s√©quences de texte en sortie. Leur caract√©ristique principale est leur capacit√© √† conserver une forme de m√©moire gr√¢ce √† des boucles r√©currentes au sein de leurs couches cach√©es. Cela permet √† l'information contextuelle de circuler √† travers le r√©seau, de sorte que les sorties pr√©c√©dentes puissent influencer les calculs effectu√©s √† chaque √©tape temporelle.

Cette capacit√© √† m√©moriser les informations contextuelles est similaire √† notre fa√ßon de lire. Lorsque nous lisons, nous retenons les √©l√©ments importants des mots et des phrases pr√©c√©dentes pour les utiliser comme r√©f√©rence et comprendre le sens des nouveaux mots et des nouvelles phrases.
Les LSTM (Long Short-Term Memory) sont une variante des RNN. En effet, bien que les RNN soient capables de conserver des informations sur de courtes p√©riodes de temps, ils ont du mal √† conserver des informations sur de longues p√©riodes de temps. C'est ce que l'on appelle le probl√®me de la disparition du gradient. Les LSTM peuvent donc √™tre consid√©r√©s comme des RNN am√©lior√©s. Les LSTM sont compos√©s de trois portes : la porte d'oubli, la porte d'entr√©e et la porte de sortie. La porte d'oubli permet de d√©cider quelles informations doivent √™tre oubli√©es ou conserv√©es. La porte d'entr√©e permet de d√©cider quelles nouvelles informations doivent √™tre stock√©es dans la cellule d'√©tat. Enfin, la porte de sortie permet de d√©cider quelles informations doivent √™tre renvoy√©es en sortie. Ainsi, gr√¢ce √† leur structure sp√©cifique, les LSTM sont capables de capturer et de retenir des informations pertinentes sur une longue s√©quence, permettant ainsi de conserver la relation entre les mots √† travers diff√©rentes parties de la s√©quence. Ils sont donc particuli√®rement adapt√©s √† la traduction de textes longs. 

Cependant, une r√©volution majeure dans le domaine de la traduction automatique est survenue avec l'av√®nement des Transformers. Contrairement aux LSTM, qui sont bas√©s sur des architectures r√©currentes, les Transformers utilisent une approche bas√©e sur l'attention pour capturer les relations entre les mots. Cette architecture r√©volutionnaire a permis de surmonter les limitations des LSTM en termes de gestion des d√©pendances √† long terme et de parall√©lisme. Les Transformers se sont impos√©s comme une avanc√©e majeure dans la traduction automatique en raison de leur capacit√© √† traiter efficacement les s√©quences de texte et √† capturer les d√©pendances contextuelles sur de plus longues distances. Gr√¢ce √† leur m√©canisme d'attention, les Transformers peuvent attribuer des poids diff√©rents aux mots en fonction de leur importance, ce qui leur permet de comprendre le contexte global de la phrase et d'effectuer des traductions plus pr√©cises et coh√©rentes.

C'est pourquoi, j'ai donc d√©cid√© d'utiliser des mod√®les de traduction neuronaux bas√©s sur les Transformers. Afin de choisir quels mod√®les j'allais utiliser et finetuner, j'ai √©t√© lire le cours sur la traduction de Hugging Face. Plusieurs mod√®les √©taient propos√©s, comme par exemple T5, MarianMT, Bart...
T5 (Text-To-Text Transfer Transformer) est un mod√®le de traitement du langage naturel d√©velopp√© par Google. Il s'agit d'un mod√®le bas√© sur les Transformers qui peut √™tre utilis√© pour diverses t√¢ches, dont la traduction. 
Bart (Bidirectional and AutoRegressive Transformers") est un mod√®le d√©velopp√© par Facebook AI. Il est lui aussi bas√© sur les Transformers et a √©t√© sp√©cifiquement con√ßu pour la g√©n√©ration de texte. 
MarianMT est un projet open-source qui se concentre sur la traduction automatique neuronale. Il s'agit d'une impl√©mentation bas√©es sur les Transformers. MarianMT utilise le m√™me mod√®le de base que Bart avec quelques modifications.

Souhaitant utiliser des mod√®les sp√©cialis√©s dans les t√¢ches de traduction neuronaux bas√©s sur les Transformers, j'ai donc d√©cid√© de finetuner des mod√®les bas√©s sur MarianMT.

### 3.2.2 Les donn√©es n√©cessaires √† l'entra√Ænement des mod√®les

Souhait faire de la traduction de l'anglais vers le fran√ßais et du fran√ßais vers l'anglais, j'ai choisi de finetuner les deux mod√®les suivants :
