# dishdecoder
üçΩÔ∏è

### RAPPORT SUR LE PROJET DISHDECODER

PLAN :

I. Introduction

    1.1 Contexte
    1.2 Objectifs
    1.3 Approche de la solution
    1.4 Technologies utilis√©es
    1.5 Sch√©ma de l‚Äôapplication

II. ETAT DE L‚ÄôART
    Introduction
    1. Contexte historique
    2. Les fondamentaux des Transformers
    3. Architecture des Transformers
    4. Domaines d'applications des Transformers
    5. Avanc√©es r√©centes et d√©fis
    Conclusion

III. DISHDECODER

    3.1 Le mod√®le d'OCR
        3.1.1 Le choix du mod√®le
        3.1.2 Les donn√©es n√©cessaires √† l‚Äôentra√Ænement du mod√®le
        3.1.3 La cr√©ation du dataset
        3.1.4 Le pr√©traitement des donn√©es
        3.1.5 Le stockage du dataset
        3.1.6 L‚Äôentra√Ænement du mod√®le
        3.1.7 Configuration des hyperparam√®tres du mod√®le
        3.1.8 Comparaison des mod√®les et choix du meilleur
        3.1.9 Le stockage du mod√®le

    3.2 Les mod√®les de traduction
        3.2.1 Le choix des mod√®les
        3.2.2 Les donn√©es n√©cessaires √† l‚Äôentra√Ænement des mod√®les
        3.2.3 La cr√©ation du dataset
        3.2.4 Le pr√©traitement des donn√©es
        3.2.5 Le stockage du dataset
        3.2.6 L‚Äôentra√Ænement des mod√®les
        3.2.7 Configuration des hyperparam√®tres des mod√®les
        3.2.8 Comparaison des mod√®les et choix des meilleurs
        3.2.9 Le stockage des mod√®les

    3.3 L'utilisation d'Open AI
        3.3.1 Pourquoi le choix d'Open AI ?
        3.3.2 L'API d'Open AI
        3.3.3 Le mod√®le utilis√©
        3.3.4 Le prompt

    3.4 Le back-end
        3.4.1 L‚ÄôAPI
        3.4.2 La sch√©matisation de l‚ÄôAPI
        3.4.3 La base de donn√©es relationnelle
        3.4.4 Le stockage des images des utilisateurs

    3.5 Le front-end
        3.5.1 L'utilisation de Streamlit
        3.5.2 La sch√©matisation du front-end
        3.5.3 Les fonctionnalit√©s de l'application
        3.5.4 L'interaction avec le back-end
    
    3.6 L'organisation du projet
        3.6.1 Le Trello (GANT ?)
        3.6.2 Le Kanban
        3.6.2 Le GitHub

IV. CONCLUSION

    4.1 La r√©alisation du projet
    4.2 Les difficult√©s rencontr√©es
    4.3 Les am√©liorations et les perspectives d'√©volution
    

# I. Introduction

## 1.1. Contexte

Depuis plusieurs mois maintenant, l'Intelligence Artificielle est au coeur de l'actualit√©. Tant√¥t adul√©e, par certains qui voient en elle un moyen de r√©soudre les probl√®mes de l'humanit√©, tant√¥t d√©cri√©e, par d'autres qui la voient comme une menace pour l'emploi et la survie de l'esp√®ce, l'Intelligence Artificielle est un sujet qui fait d√©bat. 
C'est dans ce contexte que je me suis mise en qu√™te d'un projet √† r√©aliser o√π je pourrais m√™ler √† la fois mes connaissances en IA et un sujet qui est tr√®s terre √† terre : la nourriture. Les premi√®res id√©es qui me sont venues concernaient la lecture des ingr√©dients pr√©sents sur les emballages alimentaires et la d√©tection de substances dangereuses dans certains plats pr√©par√©s, cependant de nombreuses applications exitent d√©j√† sur le march√© et je ne voyais pas comment je pourrais apporter une plus-value √† ce qui existe d√©j√†.
C'est au d√©tour d'une conversation avec mon p√®re que l'id√©e de Dishdecoder m'est venue. En effet, celui-ci me demandait si je connaissais une application o√π lorsqu'on lui envoie une photo avec du texte celle-ci est capable de renvoyer une traduction de ce texte dans la langue souhait√©e. Bien sur, il existe d√©j√† des applications qui extraient du texte pr√©sent sur une image, des applications faisant de la traduction, et m√™me des applications faisant les deux comme par exemple TextGrabber, mais je me suis dit que je pourrais essayer de faire quelque chose de similaire, sp√©cialis√© dans la cuisine et plus pr√©cis√©ment dans la lecture et la traduction de recettes de cuisine. C'est ainsi que Dishdecoder est n√©.

## 1.2. Objectifs

L'objectif de Dishdecoder est d'√™tre une application permettant √† un utilisateur de prendre en photo une recette de cuisine et d'obtenir une traduction de celle-ci dans la langue de son choix. L'application doit √™tre capable de d√©tecter le texte pr√©sent sur l'image, de le reconnaitre et de le traduire. Dans un premier temps, l'application devra √™tre capable de fournir des traductions de recettes de cuisine en anglais et en fran√ßais. L'application devra √©galement proposer √† l'utilisateur de sauvegarder les recettes traduites dans un espace personnel. Enfin, l'application devra permettre √† l'administrateur de voir les traductions r√©alis√©es afin de pouvoir r√©colter des donn√©es pour am√©liorer les mod√®les de traduction.

## 1.3 Approche de la solution

Pour r√©aliser ce projet, plusieurs composants allaient √™tre n√©cessaires:
- Les mod√®les d'intelligence artificielle : celui d'extraction de texte √† partir d'image et ceux de traduction
- Les bases de donn√©es : celles contenant les datasets pour entrainer les mod√®les et celle contenant les donn√©es de l'application
- La mise en place du back-end de l'application : l√† o√π les mod√®les communiqueront avec l'API
- La r√©alisation du front-end de l'application : l√† o√π l'utilisateur pourra interagir avec l'application qui elle m√™me inter√©agira avec l'API

## 1.4 Technologies utilis√©es

Pour r√©aliser ce projet, j'ai utilis√© les technologies suivantes :
- Python : comme langage de programmation
- Pytorch : pour la r√©alisation des mod√®les d'intelligence artificielle
- FastAPI : pour la r√©alisation de l'API
- SQLModel : pour la r√©alisation de la base de donn√©es relationnelle
- Streamlit : pour la r√©alisation du front-end de l'application
- HuggingFace Hub : pour le stockage des mod√®les d'intelligence artificielle et des datasets
- Cloudinary : pour le stockage d'images
- PadddleOCR : pour la r√©alisation du mod√®le d'extraction de texte √† partir d'image
- HuggingFace : pour la r√©alisation du mod√®le de traduction
- OpenAI : pour l'utilisation de leur API 
- Playground AI : pour la r√©alisation de l'√©g√©rie de l'application


## 1.5 Sch√©ma de l'application

INSERER ICI LE SCH√âMA DE L'APPLICATION

# II. ETAT DE L'ART

Introduction :

Au cours des derni√®res ann√©es, les transformers ont r√©volutionn√© le domaine de l'apprentissage automatique et du traitement du langage naturel. Ces architectures neurales puissantes ont ouvert de nouvelles perspectives en mati√®re de mod√©lisation des s√©quences, avec des performances impressionnantes dans des t√¢ches de NLP telles que la traduction automatique, la g√©n√©ration de texte ou encore la compr√©hension du langage.

Les transformers ont √©merg√© en 2017 avec la publication de l'article "Attention is All You Need" par Vaswani et al. D√®s lors, ils ont gagn√© en popularit√© et sont devenus la r√©f√©rence en mati√®re de mod√©lisation de s√©quences. Ils se sont av√©r√©s particuli√®rement efficaces pour traiter des t√¢ches impliquant des relations √† longue distance entre deux s√©quences et une compr√©hension contextuelle approfondie.

Contrairement aux approches pr√©c√©dentes bas√©es sur les r√©seaux de neurones r√©currents (RNN) ou les r√©seaux de neurones convolutifs (CNN), les transformers se distinguent par l'utilisation d'un m√©canisme d'attention. Ce m√©canisme permet aux transformers de prendre en compte les d√©pendances contextuelles √† travers toute la s√©quence d'entr√©e, sans la n√©cessit√© d'une propagation s√©quentielle de l'information.

Dans les transformers, l'attention permet de calculer des pond√©rations sur les diff√©rentes parties de la s√©quence d'entr√©e, en mettant l'accent sur les √©l√©ments les plus pertinents pour la t√¢che en cours. Cela permet aux transformers de capturer les relations √† long terme et de mod√©liser des d√©pendances complexes, ce qui en fait des mod√®les tr√®s adapt√©s aux t√¢ches de compr√©hension et de g√©n√©ration de s√©quences.

Dans cet √©tat de l'art, nous explorerons en d√©tail les principes fondamentaux des transformers, leur architecture, ainsi que les avanc√©es r√©centes dans ce domaine. Nous mettrons en √©vidence les performances remarquables des transformers dans des t√¢ches cl√©s telles que la traduction automatique et la g√©n√©ration de texte. Nous examinerons √©galement les diff√©rentes variantes de transformers qui ont √©t√© propos√©es pour am√©liorer les performances et r√©pondre √† des d√©fis sp√©cifiques. Nous explorerons √©galement l'√©volution des transformers et mettrons en √©vidence leur impact significatif sur le traitement du langage naturel et d'autres t√¢ches de mod√©lisation de s√©quences.

1. Contexte historique :

Depuis quelques ann√©es, les entreprises utilisent de plus en plus les mod√®les de traduction neuronaux. Ces mod√®les sont des mod√®les de traduction automatique qui utilisent des r√©seaux de neurones artificiels. Ils sont capables de traduire des textes d'une langue √† une autre en s'appuyant d'avantage sur le contexte. Parmi ces architectures, on retrouve notamment les RNN et les LSTM.

Les r√©seaux de neurones r√©currents (RNN) sont des mod√®les con√ßus pour traiter des s√©quences de texte en entr√©e et/ou g√©n√©rer des s√©quences de texte en sortie. Leur caract√©ristique principale est leur capacit√© √† conserver une forme de m√©moire gr√¢ce √† des boucles r√©currentes au sein de leurs couches cach√©es. Cela permet √† l'information contextuelle de circuler √† travers le r√©seau, de sorte que les sorties pr√©c√©dentes puissent influencer les calculs effectu√©s √† chaque √©tape temporelle.
Cette capacit√© √† m√©moriser les informations contextuelles est similaire √† notre fa√ßon de lire. Lorsque nous lisons, nous retenons les √©l√©ments importants des mots et des phrases pr√©c√©dentes pour les utiliser comme r√©f√©rence et comprendre le sens des nouveaux mots et des nouvelles phrases.

Les LSTM (Long Short-Term Memory) sont une variante des RNN. En effet, bien que les RNN soient capables de conserver des informations sur de courtes p√©riodes de temps, ils ont du mal √† conserver des informations sur de longues p√©riodes de temps. C'est ce que l'on appelle le probl√®me de la disparition du gradient. Les LSTM peuvent donc √™tre consid√©r√©s comme des RNN am√©lior√©s. Les LSTM sont compos√©s de trois portes : la porte d'oubli, la porte d'entr√©e et la porte de sortie. La porte d'oubli permet de d√©cider quelles informations doivent √™tre oubli√©es ou conserv√©es. La porte d'entr√©e permet de d√©cider quelles nouvelles informations doivent √™tre stock√©es dans la cellule d'√©tat. Enfin, la porte de sortie permet de d√©cider quelles informations doivent √™tre renvoy√©es en sortie. Ainsi, gr√¢ce √† leur structure sp√©cifique, les LSTM sont capables de capturer et de retenir des informations pertinentes sur une longue s√©quence, permettant ainsi de conserver la relation entre les mots √† travers diff√©rentes parties de la s√©quence. Ils sont donc particuli√®rement adapt√©s √† la traduction de textes longs. 

Cependant, une r√©volution majeure dans le domaine de la traduction automatique est survenue en 2017 avec l'av√®nement des Transformers. Contrairement aux LSTM, qui sont bas√©s sur des architectures r√©currentes, les Transformers utilisent une approche bas√©e sur l'attention pour capturer les relations entre les mots. Cette architecture r√©volutionnaire a permis de surmonter les limitations des LSTM en termes de gestion des d√©pendances √† long terme et de parall√©lisme. Les Transformers se sont impos√©s comme une avanc√©e majeure dans la traduction automatique en raison de leur capacit√© √† traiter efficacement les s√©quences de texte et √† capturer les d√©pendances contextuelles sur de plus longues distances. Gr√¢ce √† leur m√©canisme d'attention, les Transformers peuvent attribuer des poids diff√©rents aux mots en fonction de leur importance, ce qui leur permet de comprendre le contexte global de la phrase et d'effectuer des traductions plus pr√©cises et coh√©rentes. Ce m√©canisme a √©t√© une avanc√©e majeure dans le domaine du traitement du langage naturel et a permis d'obtenir des performances significativement meilleures dans des t√¢ches telles que la traduction automatique.

Depuis la publication de l'article fondateur, de nombreuses variantes et am√©liorations des transformers ont √©t√© propos√©es, telles que les transformers √† plusieurs t√™tes, les transformers r√©currents, les transformers pr√©-entra√Æn√©s et les transformers ax√©s sur la vision. Ces avanc√©es ont √©largi le champ d'application des transformers √† diff√©rents domaines et ont contribu√© √† leur popularit√© croissante.

Dans la prochaine section, nous examinerons en d√©tail les fondements des transformers, en explorant leur architecture et leur fonctionnement. Nous mettrons √©galement en √©vidence les applications les plus courantes des transformers dans le domaine du traitement du langage naturel et au-del√†.

2. Les fondements des Transformers :

Les transformers reposent sur deux principaux concepts fondamentaux : le m√©canisme d'attention et les couches d'encodeur-d√©codeur. Ces √©l√©ments cl√©s sont √† la base de l'architecture des transformers et contribuent √† leur puissance et √† leur efficacit√© dans la mod√©lisation des s√©quences.

Comme nous l'avons vu pr√©c√©demment, le m√©chanisme d'attention est l'√©l√©ment novateur central pour les transformers. En effet, cela permet aux transformers de calculer des pond√©rations pour chaque √©l√©ment de la s√©quence, en mettant l'accent sur les parties les plus pertinentes pour la t√¢che en cours. Cela diff√®re des approches s√©quentielles traditionnelles o√π l'information est propag√©e de mani√®re lin√©aire √† travers la s√©quence.

Le m√©canisme d'attention des transformers se compose de trois √©l√©ments cl√©s : les requ√™tes (queries), les cl√©s (keys) et les valeurs (values). Les requ√™tes repr√©sentent les √©l√©ments que le mod√®le souhaite encoder ou pr√©dire, les cl√©s repr√©sentent les √©l√©ments de la s√©quence d'entr√©e et les valeurs repr√©sentent les informations associ√©es √† chaque √©l√©ment de la s√©quence. Les poids d'attention sont calcul√©s en comparant les similarit√©s entre les requ√™tes et les cl√©s, puis en utilisant ces poids pour pond√©rer les valeurs.

Une caract√©ristique importante des transformers est qu'ils peuvent calculer les poids d'attention pour tous les √©l√©ments de la s√©quence en parall√®le. Cela permet des calculs plus rapides et efficaces par rapport aux approches s√©quentielles. De plus, les transformers utilisent des m√©canismes d'attention multiples, appel√©s "t√™tes d'attention", qui permettent de capturer diff√©rents types de relations et d'informations contextuelles.

Les transformers ont √©galement introduit l'id√©e de l'apprentissage par transfert, o√π les mod√®les pr√©-entra√Æn√©s sur de grandes quantit√©s de donn√©es peuvent √™tre adapt√©s √† des t√¢ches sp√©cifiques apr√®s avoir subi un l√©ger ajustement (finetuning). Cela a permis de tirer profit des mod√®les pr√©-entra√Æn√©s sur de vastes corpus de texte et a grandement contribu√© aux excellentes performances des transformers dans diverses t√¢ches de NLP.

L'architecture des transformers est bas√©e sur des couches d'encodeur-d√©codeur. L'encodeur est responsable de la compr√©hension de la s√©quence d'entr√©e, tandis que le d√©codeur g√©n√®re la s√©quence de sortie. Chaque couche d'encodeur et de d√©codeur est compos√©e de plusieurs sous-modules, tels que les couches d'attention et les r√©seaux de neurones enti√®rement connect√©s. Cette architecture d'encodeur-d√©codeur permet une compr√©hension compl√®te des s√©quences d'entr√©e et une g√©n√©ration pr√©cise des s√©quences de sortie.

Dans la prochaine section, nous allons explorer en d√©tail l'architecture des transformers.

3. Architecture des Transformers :

L'architecture des transformers est caract√©ris√©e par une structure en couches qui permet de capturer les d√©pendances √† longue distance dans les s√©quences. Chaque couche du transformer est compos√©e de deux principaux modules : l'encodeur et le d√©codeur.

L'encodeur est responsable de la repr√©sentation initiale des donn√©es d'entr√©e, tandis que le d√©codeur g√©n√®re les pr√©dictions √† partir de cette repr√©sentation encod√©e. Les transformers sont g√©n√©ralement utilis√©s dans des t√¢ches de g√©n√©ration de s√©quences, telles que la traduction automatique ou la g√©n√©ration de texte, o√π l'entr√©e et la sortie sont des s√©quences de symboles.

Chaque module d'encodeur et de d√©codeur est lui-m√™me compos√© de plusieurs couches d'attention, de couches de transformation positionnelle et de couches de feed-forward. La combinaison de ces diff√©rentes couches permet aux transformers de mod√©liser les relations contextuelles complexes et de g√©n√©rer des pr√©dictions pr√©cises.

Chaque couche de l'encodeur et du d√©codeur contient des sous-couches r√©siduelles et des couches de normalisation. Les sous-couches r√©siduelles permettent de conserver les informations non alt√©r√©es √† travers les diff√©rentes transformations, facilitant ainsi la propagation du gradient lors de l'entra√Ænement du mod√®le. Les couches de normalisation garantissent une stabilit√© dans l'apprentissage en normalisant les activations √† chaque √©tape.

Les couches d'attention sont le c≈ìur de l'architecture des transformers. Elles permettent aux mod√®les de calculer des pond√©rations sur les diff√©rentes parties de la s√©quence d'entr√©e, en mettant l'accent sur les √©l√©ments les plus pertinents. Les couches d'attention fonctionnent en calculant des scores d'attention entre chaque paire d'√©l√©ments dans la s√©quence, puis en les pond√©rant pour obtenir une repr√©sentation pond√©r√©e. Cette repr√©sentation pond√©r√©e est ensuite utilis√©e pour calculer les sorties de chaque couche.

Les couches de transformation positionnelle sont utilis√©es pour encoder l'information sur l'ordre s√©quentiel des √©l√©ments. Elles ajoutent des informations sur la position relative des √©l√©ments, permettant ainsi aux transformers de prendre en compte l'ordre s√©quentiel lors de la g√©n√©ration des pr√©dictions. Cela est particuli√®rement important dans les t√¢ches de g√©n√©ration de s√©quences o√π l'ordre des symboles est essentiel.

Les couches de feed-forward sont des couches enti√®rement connect√©es qui introduisent de la non-lin√©arit√© dans le mod√®le. Elles permettent de transformer les repr√©sentations des √©l√©ments en utilisant des op√©rations lin√©aires suivies d'une fonction d'activation. Ces transformations non lin√©aires aident √† capturer des relations complexes et √† am√©liorer la capacit√© de mod√©lisation des transformers.

Les transformers utilisent √©galement des m√©canismes de "masking" pour s'assurer que les pr√©dictions ne d√©pendent que des √©l√©ments pr√©c√©dents de la s√©quence de sortie lors de la g√©n√©ration de chaque √©l√©ment. Cela permet d'√©viter toute d√©pendance vis-√†-vis des √©l√©ments futurs, ce qui est essentiel pour les t√¢ches de pr√©diction et de g√©n√©ration s√©quentielles.

Dans les architectures des transformers, l'information se propage de mani√®re parall√®le plut√¥t que s√©quentielle. Chaque couche traite l'ensemble de la s√©quence en parall√®le, ce qui permet d'exploiter efficacement les capacit√©s de calcul parall√®le des processeurs modernes et d'acc√©l√©rer l'entra√Ænement et l'inf√©rence.

L'architecture des transformers peut √™tre √©tendue pour inclure des variantes telles que les transformers √† plusieurs t√™tes. Les transformers √† plusieurs t√™tes permettent de calculer diff√©rentes pond√©rations d'attention pour diff√©rentes parties de l'entr√©e, ce qui permet au mod√®le de mettre l'accent sur diff√©rentes relations et caract√©ristiques. Cela am√©liore la capacit√© du mod√®le √† capturer des informations sp√©cifiques et √† mod√©liser des t√¢ches complexes.

Dans la section suivante, nous allons mettre en lumi√®re les utilisations de transformers les plus populaires.

4. Domaines d'application des Transformers :

Les transformers ont √©t√© largement utilis√©s dans divers domaines pour des t√¢ches de mod√©lisation de s√©quences. Leur capacit√© √† capturer les d√©pendances √† longue distance et √† g√©n√©rer des pr√©dictions pr√©cises en fait une architecture polyvalente et performante. Voici quelques-unes des applications les plus courantes des transformers :

1. La traduction automatique : Les transformers ont r√©volutionn√© le domaine de la traduction automatique en offrant des performances significativement meilleures par rapport aux approches pr√©c√©dentes. Gr√¢ce √† leur capacit√© √† mod√©liser les relations contextuelles √† travers toute la s√©quence, les transformers sont capables de capturer les subtilit√©s du langage et de produire des traductions plus pr√©cises et naturelles.

2. R√©sum√© automatique : Les transformers sont √©galement utilis√©s pour g√©n√©rer des r√©sum√©s automatiques de textes. En mod√©lisant les relations entre les phrases et en identifiant les informations cl√©s, les transformers sont capables de condenser de longs textes en r√©sum√©s concis et clairs.

3. Recherche d'informations : Les transformers sont employ√©s dans les moteurs de recherche pour am√©liorer la pertinence des r√©sultats. En analysant les requ√™tes des utilisateurs et en √©valuant la similarit√© avec les documents, les transformers peuvent effectuer une recherche plus pr√©cise et fournir des r√©sultats mieux adapt√©s.

4. Chatbots et assistants virtuels : Les transformers sont utilis√©s pour cr√©er des chatbots et des assistants virtuels capables d'interagir avec les utilisateurs de mani√®re naturelle et fluide. Gr√¢ce √† leur capacit√© √† comprendre et √† g√©n√©rer du langage, les transformers permettent des conversations plus naturelles et plus efficaces.

5. Reconnaissance automatique de la parole : Les transformers ont √©galement √©t√© appliqu√©s avec succ√®s dans la reconnaissance automatique de la parole. En mod√©lisant les s√©quences audio et en capturant les relations temporelles, les transformers peuvent convertir des enregistrements audio en texte avec une grande pr√©cision.

6. Traitement du langage naturel : Les transformers sont utilis√©s pour diverses t√¢ches de traitement du langage naturel telles que la classification de texte, la g√©n√©ration de texte, la r√©ponse aux questions, la d√©tection d'entit√©s nomm√©es, etc. Leur capacit√© √† capturer les d√©pendances contextuelles leur permet de comprendre et de g√©n√©rer du langage de mani√®re plus pr√©cise. Actuellement, les transformers sont la m√©thode la plus performante pour la plupart des t√¢ches de traitement du langage naturel.

7. Vision par ordinateur : Les transformers ont √©galement √©t√© adapt√©s au domaine de la vision par ordinateur, o√π ils ont montr√© des r√©sultats prometteurs. En utilisant des architectures bas√©es sur les transformers, il est possible de capturer les relations spatiales entre les √©l√©ments d'une image ou d'une vid√©o, permettant ainsi des t√¢ches telles que la classification d'images, la d√©tection d'objets et la segmentation s√©mantique.

On remarque √† travers ces exemples que les transformers ont √©t√© utilis√©s avec succ√®s dans de nombreux domaines pour des t√¢ches de mod√©lisation de s√©quences. Leur capacit√© √† capturer les d√©pendances √† longue distance et √† g√©n√©rer des pr√©dictions pr√©cises en fait une architecture puissante et polyvalente. Les transformers continuent d'ouvrir de nouvelles perspectives et d'am√©liorer les performances dans diverses applications.

5. Avanc√©es r√©centes et d√©fis

R√©cemment, les transformers ont connu de nombreuses avanc√©es qui ont contribu√© √† am√©liorer leurs performances et √† √©tendre leur utilisation. Voici quelques-unes des avanc√©es les plus caract√©ristiques :

- Am√©liorations de l'attention : Les m√©canismes d'attention ont √©t√© am√©lior√©s pour prendre en compte des aspects tels que l'attention contextuelle et l'attention multi-modale. Ces am√©liorations permettent aux transformers de mieux mod√©liser les relations complexes entre les √©l√©ments de s√©quence et d'int√©grer des informations provenant de diff√©rents domaines, comme le langage et la vision.

- Adaptation aux domaines sp√©cifiques : Les transformers ont √©t√© adapt√©s √† des domaines sp√©cifiques en utilisant des techniques telles que le finetuning. Cela permet d'ajuster un mod√®le pr√©-entra√Æn√© sur des donn√©es √©tiquet√©es sp√©cifiques √† une t√¢che donn√©e. Cette approche a permis d'obtenir de bonnes performances m√™me avec des ensembles de donn√©es plus petits et sp√©cifiques √† un domaine. Cela a √©galement permis de rendre les transformers plus accessibles aux utilisateurs non experts.

Parmi les d√©fis actuels des transformers, on peut citer :

- Gestion de la complexit√© computationnelle : A l'√®re du r√©chauffement climatique, il est de plus en plus mal vu d'utiliser des technologies qui demandent autant de ressources. Les transformers, en particulier les mod√®les de grande √©chelle, n√©cessitent d'importantes ressources computationnelles pour l'entra√Ænement et l'inf√©rence. R√©duire la complexit√© et am√©liorer l'efficacit√© des mod√®les sans compromettre les performances reste un d√©fi √† relever.

- Interpr√©tabilit√© : Les transformers sont souvent consid√©r√©s comme des "bo√Ætes noires" en raison de leur complexit√©. Comprendre et interpr√©ter les d√©cisions prises par les transformers reste un d√©fi important. Comprendre comment les mod√®les prennent leurs d√©cisions et quelles sont les informations prises en compte est crucial, en particulier pour les applications critiques telles que la sant√© et la s√©curit√©. Les chercheurs travaillent sur des m√©thodes d'interpr√©tabilit√© pour mieux comprendre comment les informations sont trait√©es et utilis√©es dans les diff√©rentes couches des transformers.

6. Conclusion

En conclusion, les transformers ont r√©volutionn√© le domaine de l'intelligence artificielle, du domaine de la NLP, et ont ouvert de nouvelles perspectives dans le traitement de s√©quences. Leur architecture bas√©e sur l'attention leur conf√®re une capacit√© unique √† capturer les d√©pendances √† longue distance, ce qui les rend particuli√®rement adapt√©s pour mod√©liser des donn√©es s√©quentielles complexes. Les transformers ont √©t√© largement utils√©s dans des domaines tels que la traduction automatique, le r√©sum√© de texte, la vision par ordinateur et bien d'autres.

Au fil des ann√©es, les transformers ont connu des am√©liorations significatives, avec des mod√®les de plus en plus grands et des avanc√©es dans l'efficacit√© de l'attention. Cependant, des d√©fis subsistent, notamment en termes d'explicabilit√© et d'interpr√©tabilit√© des pr√©dictions, ainsi que de gestion des ressources n√©cessaires pour entra√Æner et d√©ployer des mod√®les de grande √©chelle.

Malgr√© ces d√©fis, les transformers continuent de repousser les limites de l'intelligence artificielle et d'am√©liorer les performances dans de nombreux domaines. Leur capacit√© √† mod√©liser les d√©pendances complexes des s√©quences en fait une architecture puissante et d√©sormais incontournable. 

# III. DISHDECODER

## 3.1 Le mod√®le d'OCR

### 3.1.1 Le choix du mod√®le

Il existe √† l'heure actuelle de tr√®s nombreux mod√®les pour r√©aliser de l'OCR (Optical Character Recognition).

Lors de mon alternance j'ai √©t√© en charge de la r√©alisation d'un mod√®le d'OCR pour une application de lecture de documents. J'ai donc eu l'occasion de tester plusieurs mod√®les d'OCR et de voir leurs avantages et leurs inconv√©nients. Parmi les mod√®les que j'ai test√©, il y avait Tesseract, EasyOCR, TROCR et PaddleOCR. 

Tesseract, qui est sans doute le plus connu, a √©t√© d√©velopp√© √† l'origine par Hewlett-Packard Labs et d√©sormais maintenu par Google. Tesseract est un mod√®le open source et est de base fait pour √™tre utilis√© en language C++ mais il existe des wrappers pour l'utiliser en Python, comme par exemple Pytesseract. Cependant, apr√®s test, Tesseract n'est pas le mod√®le le plus performant, notamment sur les images avec du bruit. De plus, il n'est pas toujours simple √† configurer selon l'utilisation que l'on souhaite en faire et aussi son impl√©mentation en Python peut se r√©veler tr√®s compliqu√©e en raison d'incompatibilit√©s avec d'autres librairies. C'est pourquoi, lors de mon alternance, j'ai d√©cid√© de ne pas utiliser Tesseract.

EasyOCR est quant √† lui connu pour √™tre une librairie open-source, d√©velop√©e par JaidedAi, qui prend en charge plus de 70 langues et qui est compatible avec de nombreux langages de programmation dont Python, C#, Java. En plus de cela, EasyOCR est tr√®s rapide. Par ailleurs, comme son nom l‚Äôindique EasyOCR est facile √† utiliser. Il suffit de trois lignes, dont l'importation, pour qu‚Äôil lise une image. Cependant, j'avais remarqu√© que les sorties de celui-ci n‚Äô√©taient pas toujours exactes. Que ce soit pour la lecture des chiffres pr√©sents sur les documents ou la lecture des diff√©rentes zones de texte, de nombreuses erreurs √©taient r√©alis√©es √† chaque lecture : confusion entre deux caract√®res, oubli d‚Äôespace, ajout de caract√®res etc. Malgr√© divers r√©glages dans les param√®tres, les r√©sultats restaient toujours brouillons.

TROCR (Tokenization, Recognizing, and OCR) est √©galement une librairie open-source, datant de 2017, d√©velopp√©e par Microsoft et impl√©ment√©e
dans Transformers, la librairie open-source d‚ÄôHugging Face. TROCR combine les √©tapes d‚ÄôOCR pour extraire le texte des images, et de tokenisation pour convertir le texte en une s√©quence de tokens utilisable par des mod√®les de traitement du langage naturel. Contrairement √† EasyOCR, le traitement des images via TROCR est beaucoup plus long. Malgr√© cela j'ai quand m√™me test√© TROCR, avec le mod√®le base-printed qui est cens√© √™tre le mod√®le le plus adapt√© √† la lecture de caract√®res d‚Äôimprimerie. En effet, le mod√®le base-printed a √©t√© fine-tun√© sur le dataset SROIE qui est constitu√© d‚Äôun millier de tickets de caisse. Apr√®s de nombreux tests, les r√©sultats obtenus avec TROCR √©taient g√©n√©ralement meilleurs que ceux obtenus avec EasyOCR. Toutefois, TROCR donnait exclusivement des sorties en majuscules, la ponctuation √©tait approximative, les espaces pas toujours respect√©s et surtout, le temps d‚Äôinf√©rence du mod√®le √©tait beaucoup trop important. TROCR doit √™tre r√©serv√© √† un usage sur GPU, ce qui n'√©tait pas possible lors de mon altenance et qui ne l'est toujours pas aujourd'hui avec Dishdecoder.

PaddleOCR est, comme pour les trois OCR pr√©c√©dents, une librairie open-source. Celle-ci est d√©velopp√©e par PaddlePaddle (PArallel Distributed Deep LEarning), une entreprise chinoise. Elle utilise des algorithmes de pointe pour d√©tecter, localiser et reconna√Ætre des caract√®res dans des images, et est disponible dans de
nombreuses langues. L‚Äôutilisation de PaddleOCR ressemble √† celle d‚ÄôEasyOCR par sa facilit√©. L√† aussi, la librairie est compatible avec plusieurs langages de programmation tels que Python, C++ et Java, ce qui facilite son int√©gration dans diff√©rents projets.
L‚Äôarchitecture de deep-learning utilis√©e derri√®re PaddleOCR est un CRNN : Convolutional Recurrent Neural Network. Le CRNN est un r√©seau de neurones profonds qui combine √† la fois un CNN (Convolutional Neural Network) et un RNN (Recurrent Neural Network). 
Dans un CRNN, la couche de convolution, CNN, est utilis√©e pour extraire des caract√©ristiques pertinentes des images. Ensuite, la couche RNN est utilis√©e pour analyser et traiter ces caract√©ristiques dans un contexte de s√©quence. Les sorties de la couche de convolution sont
transform√©es en s√©quences d'entr√©e pour le RNN, qui utilise sa m√©moire interne pour prendre en compte les caract√©ristiques pr√©c√©dentes tout en analysant la s√©quence actuelle. Cela permet au CRNN de mieux comprendre la structure de l'image et d'extraire des informations textuelles
pertinentes.
Concernant PaddlePaddle, il s‚Äôagit de la premi√®re plateforme open-source ind√©pendante de deep learning en Chine. Elle est d√©velopp√©e par l‚Äôentreprise Baidu. PaddlePaddle est plus ou moins l‚Äô√©quivalent chinois de TensorFlow. La plateforme PaddlePaddle est r√©put√©e pour sa rapidit√© d‚Äôinf√©rence, sa capacit√© √† traiter des volumes de donn√©es massifs et son large √©cosyst√®me de modules et d'outils int√©gr√©s. La plateforme est largement utilis√©e dans diff√©rents domaines, notamment l'industrie, le transport, l‚Äôagriculture et les services publics. 
D√®s la premi√®re utilisation de PaddleOCR, avec le mod√®le de base en langue fran√ßaise et latine, j‚Äôai  pu observer que les sorties donn√©es √©taient bien meilleures que celles d‚ÄôEasyOCR. Certains points √©taient malgr√© tout perfectibles : les espaces entre les caract√®res n‚Äô√©taient souvent pas retranscrits et il y avait parfois de la confusion entre certains caract√®res. Cependant, il √©tait mis en avant sur le GitHub de PaddleOCR qu'avec du finetuning, les r√©sultats pouvaient √™tre nettement am√©lior√©s. 

Voici un exemple de test que j'ai r√©alis√© lors de mon alternance sur EasyOCR, TROCR et PaddleOCR :

INSERER ICI LES IMAGES MR OU MME BERTIN


Comme nous pouvons l'observer, les r√©sultats obtenus par TROCR et PaddleOCR sont bien concluants que ceux obtenus par EasyOCR. 

C'est donc pour ces diverses raisons que lors de mon alternance j'ai d√©cid√© de finetuner PaddleOCR. Les bons r√©sultats obtenus font que mon mod√®le finetun√© est d√©sormais le mod√®le utilis√© par mon entreprise d'alternance lorsqu'il s'agit de faire de l'OCR.

Ne pouvant r√©utiliser le mod√®le et le dataset d'entra√Ænement de mon alternance pour Dishdecoder, j'ai choisi de reconstituer un nouveau dataset d'entra√Ænement et de finetuner un mod√®le de PaddleOCR sur ce nouveau dataset.

### 3.1.2 Les donn√©es n√©cessaires √† l'entra√Ænement du mod√®le

Avant de savoir quelles donn√©es sont n√©cessaires pour finetuner un mod√®le de PaddleOCR, il est important de comprendre comment fonctionne PaddleOCR.
En effet, il n'existe pas qu'un seul mod√®le PaddleOCR qui sait g√©rer toutes les langues. Il existe un mod√®le par langue. Par exemple, il existe un mod√®le pour la langue fran√ßaise, un mod√®le pour la langue anglaise, un mod√®le pour la langue chinoise etc. Il y a √©galement plusieurs versions, il y a par exemple les mod√®le PP-OCRv2 de l'ancienne g√©n√©ration et les PP-OCRv3 qui sont les plus r√©cents et les plus performants. A vitesse √©gale, selon les langues, les mod√®les PP-OCRv3 sont entre 5% et 11% plus pr√©cis (accuracy) que les mod√®les PP-OCRv2.
Par ailleurs, PaddleOCR propose d‚Äôutiliser trois types de mod√®les diff√©rents qui peuvent √™tre combin√©s lors de la lecture d‚Äôune image. Il y a un mod√®le de reconnaissance de caract√®res (recognition model aussi appel√© rec), un mod√®le de d√©tection de texte (detection model aussi appel√© det) et un mod√®le de classification d‚Äôangle du texte (classification model aussi appel√© cls). Dans mon cas d‚Äôusage, je n‚Äôavais pas besoin de finetuner mon mod√®le pour qu‚Äôil soit plus performant dans la d√©tection de texte ou dans la reconnaissance de l‚Äôorientation du texte. Je souhaitais uniquement avoir un mod√®le plus performant sur la reconnaissance de caract√®res, c‚Äô√©tait donc un recognition model que j‚Äôallais finetuner. Par ailleurs, plut√¥t que de finetuner le mod√®le de reconnaissance fran√ßais, french_mobile_v2.0_rec, j‚Äôai choisi de finetuner le mod√®le de reconnaissance latin appel√©
latin_PP-OCRv3_rec. En effet, le mod√®le latin fait partie des mod√®les de 3√®me g√©n√©ration, ce qui implique de meilleures performances.

Il est recommand√©, sur le GitHub de PaddleOCR, d'utiliser plus de 5000 images lors du finetuning d'un mod√®le de reconnaissance. 

D'experience, je sais √©galement que plus le dataset comprend des images diverses et vari√©es et plus le mod√®le finetun√© est performant. Il √©tait donc n√©cessaires que je receuille des images avec diff√©rentes tailles, diff√©rentes polices de caract√®res, diff√©rents types de fonds, diff√©rentes couleurs, diff√©rentes longueur de texte, des majuscules, des minuscules, des chiffres, des caract√®res sp√©ciaux etc.

Souhaitant √©galement que mon OCR soit performant sur du fran√ßais comme sur de l'anglais, j'ai d√©cid√© de r√©colter des images en fran√ßais et en anglais.

Pour finetuner un mod√®le de reconnaissance PaddleOCR, le dataset doit √™tre constitu√© de la mani√®re suivante : il doit y avoit des images et un fichier texte. Le fichier texte doit contenir deux √©l√©ments pour chaque image: le chemin de sa localisation avec le nom donn√© √† l'image et le texte pr√©sent sur l'image. Le chemin de l'image et le texte pr√©sent sur l'image doivent √™tre s√©par√©s par une tabulation. 

### 3.1.3 La cr√©ation du dataset

N'ayant trouv√© aucun dataset pertinent r√©pondant √† mes besoins, j'ai d√©cid√©, comme lors de mon alternance, de cr√©er mon propre dataset.
Pour cela, aucun outil de labellisation n'est n√©cessaire. 
En effet, le seul outil dont j'avais besoin √©tait un outil de capture d'√©cran. J'ai donc utilis√© l'outil de capture d'√©cran int√©gr√© √† mon ordinateur, celui de Ubuntu.
Afin de constitu√© mon dataset, j'ai √©t√© sur de nombreux sites internet dont : 
- Amazon.com o√π j'ai recherch√© diff√©rents ouvrages en fran√ßais et en anglais. J'y ai captur√© des images de titres de livres, de quatri√®me de couverture, de table des mati√®res...
- Brest.fr o√π j'ai pu trouver certains extraits de magazines, des infographies, des affiches, des publicit√©s...
- Des sites internet d'archive o√π sont scann√©s d'anciens num√©ros de journaux actuels comme Le Monde ou Le Canard Encha√Æn√©, o√π j'ai pu capturer des images de titres d'articles etc.
- Des sites internet sp√©cialis√©s dans les journaux et annuaires anciens pour trouver des polices d'√©criture plus rares.
- Des sites internet de restaurants dont Restaurantguru.com o√π j'ai pu capturer des images √† partir des diff√©rents menus.

J'ai r√©colt√© en tout 1285 images. Pour chaque image, j'ai recens√© son chemin avec son nom ainsi que le texte pr√©sent dans cette m√™me image, le tout consign√© dans un fichier texte.

### 3.1.4 Le pr√©traitement des donn√©es

Parmi les images r√©cup√©r√©es, j'en ai s√©lectionn√© 565 pour l'entra√Ænement et 720 pour la validation.
Afin de rendre mon dataset plus robuste, j'ai fait de la data augmentation sur mon dataset d'entrainement. 
Pour cela, j'ai adapt√© le code du projet Straug pr√©sent sur le GitHub de Roatenzia √† ce dont j‚Äôavais besoin.
J‚Äôai choisi d‚Äôappliquer √† mes images les effets suivants : GaussianNoise, SpeckleNoise, ImpulseNoise, MotionBlur, DefocusBlur, Shadow, Rain. 

INSRER IMAGE EXEMPLE

Une fois les nouvelles images g√©n√©r√©es, 17 nouvelles images en sortie pour 1 image en entr√©e, soit 9605 nouvelles images, j‚Äôai pu les labelliser dans mon fichier texte d‚Äôentra√Ænement et les d√©placer dans mon dossier comprenant toutes les images.
Ainsi le dataset final pour ce finetuning comprenait 10170 images pour l‚Äôentra√Ænement et 720 images pour la validation.
L'ensemble du dataset a √©t√© mis dans un dossier train_data. Dans train_data il y a un dossier data o√π sont stock√©es toutes les images du dataset et deux fichiers .txt : dataset_IMG_train_REDO.txt et dataset_IMG_val_REDO.txt . Comme leurs noms le laisse supposer, le fichier dataset_IMG_train_REDO.txt contient les chemins ainsi que le texte contenu sur les images d'entra√Ænement et le fichier dataset_IMG_val_REDO.txt contient les chemins ainsi que le texte contenu sur les images de validation.

### 3.1.5 Le stockage du dataset

J'aurais souhait√© stocker mon dataset sur le Hub de Hugging Face, mais cela n'a pas pu √™tre fait car un repository sur HuggingFace ne peut pas contenir plus de 10000 √©l√©ments. J'ai donc stock√© mon dataset dans un dossier sur mon ordinateur. 

### 3.1.6 L'entrainement du mod√®le

La premi√®re √©tape avant de lancer l'entra√Ænement, fut de cloner le repository GitHub de PaddleOCR sur mon ordinateur.
```$ git clone https://github.com/PaddlePaddle/PaddleOCR.git```
Une fois le projet clon√©, il fallait, √† l‚Äôint√©rieur du dossier principal, cr√©er un dossier pretrain_models.
En effet, lorsque l‚Äôon finetune un mod√®le, il nous faut obligatoirement un mod√®le de base, mod√®le auquel on va apporter des modifications avec le finetuning. Pour les raisons expliqu√©es plus haut, j‚Äôavais choisi le mod√®le latin_PP-OCRv3_rec. Il fallait alors t√©l√©charger ce mod√®le depuis le GitHub de PaddleOCR et l‚Äôextraire dans le dossier pretrain_models.
Apr√®s cela, √† la racine du dossier principal PaddleOCR, au m√™me niveau que pour le dossier pretrain_models, il fallait copier le dossier train_data comprenant le dataset.
L‚Äô√©tape suivante √©tait de retourner dans le dossier principal PaddleOCR, puis d‚Äôaller dans le dossier configs, puis rec, puis PP-OCRv3, puis multi_language, puis de faire une copie du fichier latin_PPOCRv3_rec.yml en le renommant. C‚Äôest dans ce fichier de configuration qu‚Äôinterviennent les
modifications de chemins afin de permettre √† PaddleOCR d‚Äôutiliser le dataset contenu dans train_data lors de l‚Äôentra√Ænement. Ce fichier de configuration permet √©galement de sp√©cifier bon nombre de param√®tres : le chemin du dictionnaire √† utiliser, la prise en compte des espaces, mais
√©galement le chemin du mod√®le pr√©-entra√Æn√© √† utiliser etc.
Une fois toutes ces √©tapes r√©alis√©es, j'ai d√©cid√© de cr√©er un nouvel environnement conda d√©di√© √† l'entrainement de mon mod√®le. Pour cela, j'ai utilis√© la commande suivante : 
```$ conda create --name paddle-training python=3.10```
J'ai ensuite activ√© cet environnement avec la commande suivante : 
```$ conda activate paddle-training```
Apr√®s cela, afin d'utiliser la version GPU pour l'entra√Ænement du mod√®le, j'ai fait la commande suivante qui est recommand√©e sur le site de Paddlepaddle: 
```python -m pip install paddlepaddle-gpu==2.4.2.post117 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html```
Enfin j'ai install√©, en prenant bien soin d'√™tre √† ce moment-l√† dans le dossier PaddleOCR, les d√©pendances n√©cessaires √† l'entra√Ænement du mod√®le avec la commande suivante : 
```$ pip install -r requirements.txt```
Une fois toutes ces √©tapes r√©alis√©es, tout en √©tant toujours bien √† la racine du dossier PaddleOCR, j'ai pu lancer l'entra√Ænement de mon mod√®le avec la commande suivante : 
```$ python3 -m paddle.distributed.launch --gpus '0'  tools/train.py -c configs/rec/PP-OCRv3/multi_language/latin_PP-OCRv3_rec_4_finetuning.yml```

L'entra√Ænement, de 200 epochs, a √©t√© lanc√© √† 12h49 et s'est termin√© √† 23h18, soit un peu plus de 10h d'entra√Ænement.
Comme sp√©cifi√© dans le fichier de configuration, les poids du mod√®le finetun√© ayant la meilleure pr√©cision (accuracy) ainsi que ses diff√©rents fichiers (fichier de configuartaion, fichier qui repertorie tous les logs d'entra√Ænement, poids du mod√®le actualis√©s √† chaque epoch...) ont √©t√© stock√©s dans le dossier ./output/v3_latin_mobile_FINETUNING_1

Vis-√†-vis de la m√©thode d'entra√Ænement, Le finetuning est souvent consid√©r√© comme une forme d'entra√Ænement supervis√© car il n√©cessite des annotations √©tiquet√©es pour guider le processus d'apprentissage. Cependant, on peut √©galement le consid√©rer comme une approche semi-supervis√©e, car le mod√®le de base pr√©-entra√Æn√© apporte une connaissance pr√©alable supervis√©e qui est ensuite am√©lior√©e et affin√©e gr√¢ce aux donn√©es suppl√©mentaires annot√©es que nous lui fournissons. 

### 3.1.7 Configuration des hyperparam√®tres du mod√®le

Lors de ce premier finetuning, j'ai choisi de modifier les hyperparam√®tres suivants :
- le batch size, que j'ai mis √† 16
- le nombre d'epochs, que j'ai mis √† 200

Etant donn√© qu'apr√®s ce premier entra√Ænement j'ai eu l'occasion d'avoir acc√®s √† un GPU plus puissant, j'ai d√©cid√© de refaire un finetuning avec cette fois-ci un batch size de 128 et un nombre d'epochs de 200.
Ensuite, j'ai d√©cid√© de faire un troisi√®me finetuning avec un batch size de 64 et un nombre d'epochs de 200.
Puis, j'ai d√©cid√© de faire un quatri√®me finetuning avec un batch size de 64 et un nombre d'epochs de 750.
Enfin, j'ai d√©cid√© de faire un cinqui√®me finetuning avec un batch size de 32 et un nombre d'epochs de 200.

### 3.1.8 Comparaison des mod√®les et choix du meilleur 

N'ayant pas utilis√© d'outil de monitoring pour les entra√Ænements de mon mod√®le, je n'ai pas pu avoir acc√®s √† des graphiques repr√©sentant l'√©volution de l'accuracy au cours des entra√Ænements. Cependant, j'ai pu avoir acc√®s √† ces informations dans le fichier de log de chaque entra√Ænement. 
Par ailleurs, √† la fin de chaque entra√Ænement, PaddleOCR indique qu'elle fut l'epoch o√π a √©t√© calcul√©e la meilleure accuracy sur le dataset de validation. Pour PaddleOCR l'accuracy est la m√©trique utilis√©e pour √©valuer la performance du mod√®le.

Voici les r√©sultats obtenus pour chaque entra√Ænement :

INSERER ICI LES RESULTATS OBTENUS POUR CHAQUE ENTRAINEMENT

Comme nous pouvons le constater, le mod√®le ayant la meilleure accuracy est le premier √† avoir √©t√© entra√Æn√©. C'est donc celui-ci que nous allons utiliser pour la suite de ce projet.

Cependant, le mod√®le obtenu apr√®s l'entra√Ænement n'est pas utilisable en l'√©tat. Apr√®s entra√Ænement tous les mod√®les PaddleOCR sont compos√©s de trois fichiers : un fichier en .pdopt, un fichier en .pdparams et un fichier en .states. Dans notre cas ces trois fichiers sont stock√©s dans le dossier ./output/v3_latin_mobile_FINETUNING_1/best_accuracy. Afin de pouvoir utiliser ce mod√®le, il a donc fallu le convertir en un mod√®le utilisable par PaddleOCR.

 Pour cela, j'ai √©xecut√© la commande suivante : 
```$ python3 tools/export_model.py -c configs/rec/PP-OCRv3/multi_language/latin_PP-OCRv3_rec_4_finetuning.yml -o Global.pretrained_model=./output/v3_latin_mobile_FINETUNING_1/best_accuracy Global.save_inference_dir=./inference```

Une fois cette commande ex√©cut√©e, le mod√®le fut converti et stock√© dans le dossier inference. 
Le mod√®le est alors compos√© de trois nouveaux fichiers : inference.pdiparams, qui est le fichier de param√®tres du mod√®le, inference.pdiparams.info, qui est le fichier d'informations du mod√®le et inference.pdmodel, qui est le fichier de programme du mod√®le en lui-m√™me.
Afin de facilit√© l'utilisation du nouveau mod√®le, je cr√©e un dossier ocr_model et j'ai ajout√© dans ce dossier les trois fichiers d'inf√©rence convertis, le fichier de configuration du mod√®le en .yml ainsi que le dictionnaire utilis√© en .txt.

INSERER ICI UNE IMAGE DU DOSSIER OCR_MODEL

Afin de v√©rifier les performances du nouveau mod√®le, j'ai √©crit un script Python permettant de comparer, EasyOCR, PaddleOCR avant le finetuning, et mon mod√®le PaddleOCR apr√®s le finetuning.

Pour mesurer les performances des OCR, j‚Äôai choisi de calculer les distances de Levenshtein et d‚ÄôIndel.
La distance de Levenshtein est une mesure de similitude entre deux cha√Ænes de caract√®res, d√©finie comme le nombre minimum d'√©ditions de caract√®res simples (insertions, suppressions ou substitutions) n√©cessaires pour transformer une cha√Æne en l'autre. Plus la distance est proche de 0,
plus les deux textes sont similaires.
La distance Indel est une variation de la distance de Levenshtein qui prend en compte uniquement les insertions et les suppressions, et pas les substitutions. C'est √©galement une mesure de la dissimilarit√© entre deux cha√Ænes de caract√®res. Pour Indel, plus la distance est proche de 100, plus
les deux textes sont similaires.

Pour pouvoir comparer les OCR, j‚Äôai constitu√© un nouveau jeu de donn√©es de 10 images.


INSERER IMAGES DU DATASET DE TEST

Les r√©sultats de la comparaison furent les suivants :
- pour la distance d‚ÄôIndel (meilleur r√©sultat = proche de 100) : EasyOCR: 89.53, PaddleOCR_base : 91.76, PaddleOCR_finetun√© : 93.28 .
- pour la distance de Levenshtein (meilleur r√©sultat = proche de 0) : EasyOCR: 3.7, PaddleOCR_base: 3.0, PaddleOCR_finetun√© : 2.2 .

INSERER IMAGE AVEC LES RESULTATS    

On peut donc en conclure que peu importe la m√©thode de calcul le classement est le suivant :
ÔÉò 1√®re place PaddleOCR nouveau mod√®le finetun√©
ÔÉò 2nde place PaddleOCR mod√®le de base
ÔÉò 3√®me place EasyOCR 

Gr√¢ce √† cela, on peut en d√©duire que le finetuning a permis d‚Äôam√©liorer la pr√©cision du mod√®le. Le nouveau mod√®le finetun√© est donc l‚ÄôOCR le plus performant parmi ceux test√©s.


### 3.1.9 Le stockage du mod√®le

Afin de pouvoir utiliser ce mod√®le dans ce projet et dans d'autres √† venir, j'ai d√©cid√© de le stocker sur le Hub Hugging Face. Pour cela, j'ai cr√©√© un nouveau repository sur mon compte Hugging Face et j'ai ajout√© les fichiers du mod√®le dans ce repository.
Le mod√®le est donc maintenant disponible √† l'adresse suivante : https://huggingface.co/PaulineSanchez/PaddleOCR_ft

INSERER ICI UNE IMAGE DU HUB HUGGING FACE AVEC LE MODELE DANS LE REPOSITORY

# 3.2 Les mod√®les de traduction

### 3.2.1 Le choix des mod√®les

C'est en raison des r√©sultats √©voqu√©s dans l'√©tat de l'art de ce projet que j'ai d√©cid√© d'utiliser des mod√®les de traduction neuronaux bas√©s sur les Transformers. Afin de choisir quels mod√®les j'allais utiliser et finetuner, j'ai √©t√© lire le cours sur la traduction de Hugging Face. Plusieurs mod√®les √©taient propos√©s, comme par exemple T5, MarianMT, Bart...

T5 (Text-To-Text Transfer Transformer) est un mod√®le de traitement du langage naturel d√©velopp√© par Google. Il s'agit d'un mod√®le bas√© sur les Transformers qui peut √™tre utilis√© pour diverses t√¢ches, dont la traduction. 
Bart (Bidirectional and AutoRegressive Transformers") est un mod√®le d√©velopp√© par Facebook AI. Il est lui aussi bas√© sur les Transformers et a √©t√© sp√©cifiquement con√ßu pour la g√©n√©ration de texte. 
MarianMT est un projet open-source qui se concentre sur la traduction automatique neuronale. Il s'agit d'une impl√©mentation bas√©es sur les Transformers. MarianMT utilise le m√™me mod√®le de base que Bart avec quelques modifications.

Souhaitant utiliser des mod√®les sp√©cialis√©s dans les t√¢ches de traduction neuronaux bas√©s sur les Transformers, j'ai donc d√©cid√© de finetuner des mod√®les bas√©s sur MarianMT.

### 3.2.2 Les donn√©es n√©cessaires √† l'entra√Ænement des mod√®les

Souhait faire de la traduction de l'anglais vers le fran√ßais et du fran√ßais vers l'anglais, j'ai choisi de finetuner les deux mod√®les suivants :
`Helsinki-NLP/opus-mt-fr-en` pour la traduction du fran√ßais vers l'anglais, et `Helsinki-NLP/opus-mt-en-fr` pour la traduction de l'anglais vers le fran√ßais.
Pour finetuner ces mod√®les, j'allais avoir besoin d'un dataset contenant des phrases en anglais et leur traduction en fran√ßais. Afin d'obtenir de bonnes performances, il fallait que ce dataset soit compos√© de phrases plus ou moins longues et de phrases plus ou moins complexes. De plus, le but de ce finetuning √©tait pour moi d'obtenir des mod√®les sp√©cialis√©s dans la th√©matique de la cuisine. Il fallait donc que le dataset contienne des phrases en lien avec la cuisine. Ces phrases devaient contenir des verbes d'action propres √† la cuisine et √† la r√©alisation de recettes, des noms d'ingr√©dients, des noms de plats, des noms d'ustensiles... Par ailleurs, plus le dataset allait √™tre volumineux, plus les performances des mod√®les allaient √™tre bonnes.


### 3.2.3 La cr√©ation du dataset

De nombreux datasets de traduction sont disponibles sur les sites de Kaggle ou m√™me d'Hugging Face. Cependant, aucun ne correspondait √† mon th√®me, qui √©tait la cuisine, dans les deux langues choisies, le fran√ßais et l'anglais. Comme pour le finetuning d'OCR, j'ai donc d√©cid√© de cr√©er mon propre dataset. Pour cela, j'ai utilis√© plusieurs sites de cuisine : marmiton.org, allrecipes.com, simplyrecipes.com, mercotte.fr et quelques autres. 
A chaque fois, je selectionnais une ou plusieurs phrases qui me semblaient int√©ressantes, je les copiais, les collais dans la zone de texte du site deepl.com et je les traduisais dans la langue cible. Ayant des connaissances dans la traduction en anglais, je pouvais dans un second temps faire les modifictions n√©cessaires afin d'obtenir une traduction de qualit√©. Il ne me restait plus qu'√† copier les phrases en anglais et leur traduction en fran√ßais, et inversement, dans mon fichier CSV.
J'ai utilis√© deepl.com car il s'agit d'un site de traduction qui utilise des mod√®les neuronaux et qui est donc plus performant que Google Traduction par exemple. 

Afin de pouvoir finetuner mes mod√®les, le moyen le plus efficace √©tait de cr√©er des datasets avec la librairie Datasets de Hugging Face. Cette librairie permet de cr√©er des datasets √† partir de fichiers JSON, CSV, TXT, XML, etc. 
Il fallait donc dans un premier temps que je cr√©e un fichier CSV avec les donn√©es sur lesquelles je souhaitais finetuner mon mod√®le.
Le CSV devait avoir une colonne `id` qui s'incr√©mente √† chaque nouvelle entr√©e et une colonne `translation` qui contient la phrase en anglais et sa traduction en fran√ßais.
J'ai donc cr√©e un CSV avec ces caract√©ristiques et il ne me restait plus qu'√† le remplir avec les donn√©es que je souhaitais. 
Au final, mon fichier CSV √©tait constitu√© de 399 entr√©es. 

### 3.2.4 Le pr√©traitement des donn√©es

Avant de transformer mon fichier CSV en Dataset Hugging Face, je n'ai pas fait de data augmentation. J'ai estim√© que les donn√©es que j'avais s√©lectionn√©es √©taient suffisantes pour finetuner mes mod√®les. 
J'ai donc utilis√© la librairie Datasets de Hugging Face pour transformer mon fichier CSV en Dataset Hugging Face. Lors de l'ex√©cution du script, j'ai pris soin de pr√©ciser que la colonne `translation` √©tait une colonne de type `Translation` afin que le Dataset Hugging Face soit bien un dataset de traduction.

INSERER ICI LE CODE POUR TRANSFORMER LE CSV EN DATASET HUGGING FACE

J'ai ensuite envoy√© mon dataset sur le Hub de Hugging Face o√π il a √©t√© stock√© √† l'adresse suivante `https://huggingface.co/datasets/PaulineSanchez/recipes_translation_4_helsinki_4.0`.

Apr√®s cela, et apr√®s avoir v√©rifi√© que mon dataset √©tait correct et bien disponible sur le Hub de Hugging Face, je l'ai t√©l√©charg√©, et toujours avec la librairie Datasets de Hugging Face, j'ai divis√© mon dataset en deux datasets : un dataset d'entra√Ænement et un dataset de validation. J'ai choisi de diviser mon dataset de la mani√®re suivante, 80% des donn√©es pour l'entra√Ænement et 20% pour la validation. Cela donnait un dataset d'entra√Ænement de 319 entr√©es et un dataset de validation de 80 entr√©es.

INSERER ICI LE CODE POUR DIVISER LE DATASET EN DATASET D'ENTRAINEMENT ET DATASET DE VALIDATION

### 3.2.5 Le stockage du dataset

Comme pour le dataset avant la s√©paration en jeu d'entra√Ænement et en jeu de validation, j'ai envoy√© mon dataset sur le Hub de Hugging Face gr√¢ce √† la fonction `push_to_hub` de Hugging Face. Mon dataset a √©t√© stock√© √† l'adresse suivante `https://huggingface.co/datasets/PaulineSanchez/recipes_translation_400`. Ce dataset en affich√© en mode public afin que tout le monde puisse y avoir acc√®s et l'utiliser.

INSERER ICI LE CODE POUR ENVOYER LE DATASET SUR LE HUB DE HUGGING FACE + FLOUTER LE TOKEN
INSERER ICI UNE CAPTURE D'ECRAN DU DATASET SUR LE HUB DE HUGGING FACE

### 3.2.6 L'entra√Ænement des mod√®les

Pour entra√Æner mes mod√®les, j'ai utilis√© un script que j'ai trouv√© sur le Github de Hugging Face, dans le repository `transformers/examples/pytorch/translation`. Ce script permet d'entra√Æner un mod√®le de traduction, selon si son architecure est bas√©e sur les transformers, √† partir d'un dataset Hugging Face ou de fichiers json/CSV. J'ai donc utilis√© ce script pour entra√Æner mes deux mod√®les de traduction.
Afin de pouvoir utilis√© ce script, j'ai d√ª cloner le repository `transformers` de Hugging Face sur mon ordinateur. 
Apr√®s cela, apr√®s m'√™tre d√©plac√© dans le dossier, j'ai lanc√© la commande suivante :

```$ python examples/pytorch/translation/run_translation.py     --model_name_or_path Helsinki-NLP/opus-mt-en-fr     --do_train     --do_eval     --source_lang en     --target_lang fr     --dataset_name PaulineSanchez/recipes_translation_400  --output_dir /home/pauline/Documents/PCO/Modele_Traduction/train_hf_new_batch_4_epoch_6     --per_device_train_batch_size=6     --per_device_eval_batch_size=6     --predict_with_generate --report_to wandb --num_train_epochs 6 ```

Cette commande permet de lancer l'entra√Ænement du mod√®le de traduction. Elle prend en param√®tre le nom du mod√®le √† utiliser, le code de la langue source, le code de la langue cible, le nom du dataset √† utiliser, le chemin vers le dossier o√π stocker les r√©sultats de l'entra√Ænement, la taille du batch d'entra√Ænement, la taille du batch de validation, le nom de l'outil utilis√© pour faire du monitoring et le nombre d'epochs √† effectuer. 

J'ai donc lanc√© cette commande plusieurs fois, en inversant les langues source et cible et en pr√©cisant le nom du mod√®le √† utiliser. J'ai √©galement chang√© le nom du dossier o√π stocker les r√©sultats de l'entra√Ænement afin de pouvoir les diff√©rencier.

### 3.2.7 Configuration des hyperparam√®tres des mod√®les

Lors des diff√©rents entra√Ænements que j'ai lanc√©, les hyperparam√®tres que j'ai modifi√©s sont les suivants :
- le batch size : j'ai fait des tests avec des batch allant de 2 √† 16
- le nombre d'epochs : j'ai fait des tests avec des epochs allant de 3 √† 75

En fonction des r√©sultats que j'obtenais et des courbes dessin√©es sur Weight and Biases, j'ai pu ajuster les hyperparam√®tres au fur et √† mesure afin d'obtenir les meilleurs mod√®les possibles.

AJOUTER CAPTURE D'ECRAN DES COURBES DE W&B

### 3.2.8 Comparaison des mod√®les et choix des meilleurs

La m√©trique utilis√©e par Hugging Face pour comparer les mod√®les de traduction est le score BLEU (bilingual evaluation understudy). Le BLEU score est une m√©trique qui permet de comparer la qualit√© d'une traduction automatique √† une ou plusieurs traductions de r√©f√©rence. Il √©value la similarit√© entre les n-grammes (s√©quences de n mots cons√©cutifs) pr√©sents dans la sortie g√©n√©r√©e et ceux pr√©sents dans les r√©f√©rences. Le score est compris entre 0 et 1. Plus le score est proche de 1, plus la traduction est de bonne qualit√©. Cependant, le score BLEU pr√©sente √©galement certaines limites. Par exemple, il ne tient pas compte de la s√©mantique ou de la coh√©rence globale du texte, ce qui signifie qu'un score √©lev√© ne garantit pas n√©cessairement une traduction ou un r√©sum√© de haute qualit√© sur tous les aspects.

Ainsi, afin de pouvoir comparer les diff√©rents mod√®les entra√Æn√©s, j'ai l√† aussi utilis√© Weight and Biases. 
J'ai class√© mes mod√®les selon leur score BLEU et j'ai pu ainsi d√©terminer les meilleurs mod√®les.

INSERER ICI CAPTURE D'ECRAN DES MODELES ET DE LEUR SCORE BLEU

Ainsi, pour le mod√®le de traduction anglais-fran√ßais, le meilleur mod√®le est celui entra√Æn√© avec un batch size de 8 et 3 epochs. Pour le mod√®le de traduction fran√ßais-anglais, le meilleur mod√®le est celui entra√Æn√© avec un batch size de 12 et 4 epochs.

### 3.2.9 Le stockage des mod√®les

Une fois que j'ai d√©termin√© les meilleurs mod√®les, je les ai envoy√© sur le Hub de Hugging Face afin de pouvoir les utiliser par la suite. J'ai utilis√© la fonction `push_to_hub` de Hugging Face pour envoyer mes mod√®les sur le Hub. Il est a not√© que la fonction `push_to_hub` utilise Git et Git LFS pour envoyer les mod√®les sur le Hub. Ainsi, peu importe la taille du mod√®le, il sera envoy√© sur le Hub.
Mes mod√®les ont √©t√© stock√©s aux adresses suivantes `https://huggingface.co/PaulineSanchez/translation_for_recipes_en_fr` pour le mod√®le anglais vers fran√ßais et `https://huggingface.co/PaulineSanchez/translation_for_recipes_fr_en` pour le mod√®le fran√ßais vers anglais.
Ces mod√®les sont en mode public afin que tout le monde puisse y avoir acc√®s et les utiliser.

INSERER ICI LE CODE POUR ENVOYER LES MODELES SUR LE HUB DE HUGGING FACE + FLOUTER LE TOKEN

INSERER ICI UNE CAPTURE D'ECRAN DES MODELES SUR LE HUB DE HUGGING FACE

## 3.3 L'utilisation d'Open AI

### 3.3.1 Pouquoi le choix d'OPEN AI ?

Maintenant que mes mod√®les d'OCR et de traduction √©taient finetun√©s, j'ai souhait√© ajout√© un bloc entre les deux qui me permettrait de corriger la grammaire et l'orthographe de la sortie de l'OCR afin que le mod√®le de traduction puisse fonctionner sur un texte correct et ne pas √™tre perturb√© par des erreurs. En effet, la moindre confusion entre deux caract√®res plut√¥t proches ou m√™me l'ajout de caract√®re peut entra√Æner une erreur de traduction. Ainsi, j'ai souhait√© utiliser un mod√®le de correction orthographique et grammaticale afin de corriger les potentielles erreurs de l'OCR.

Apr√®s de nombreuses recherches, je n'ai pas r√©ussi √† trouver un package Python qui me permettrait de faire cela de la mani√®re dont je l'entends. J'ai pu tester Pygrammalecte qui est un wrapper Python autour de Grammelecte. Grammelecte est un correcteur grammatical et typographique open-source pour le fran√ßais. Il a diff√©rent modules pour √™tre ajout√© sur Google Chrome, Firefox, Libre Office, Open Office ... Cependant, sa version Python n'√©tait pas adapt√©e √† ce que je souhaitais faire. En effet, Pygrammalecte ne permet pas de corriger un texte mais seulement de donner des suggestions de correction pour un mot donn√©. Par ailleurs, de nombreux mots corrects √©taient signal√©s comme n'existant pas. Cela signifait donc que les dictionnaires utilis√©s en arri√®re plan n'√©taient pas √† jour voir erronn√©s. Je n'ai rien trouv√© d'autre qui pouurrait fonctionner pour la correction en fran√ßais. Il y a des projets qui existent mais soit ils ne fonctionnent pas, soit ils sont inachev√©s, soit ils ne sont pas utilisables en Python.

Du c√¥t√© des outils similaires √† chatGPT et Open AI, l√† aussi j'ai trouv√© des alternatives mais aucune qui puisse fonctionner aussi bien et toutes avaient un co√ªt plus √©lev√©.

### 3.3.2 L'API d'Open AI

Afin de pouvoir utiliser l'API d'Open AI, j'ai eu besoin de cr√©er un compte sur leur site. Une fois le compte cr√©e, j'ai li√© ma carte bancaire √† mon compte et d√©fini un seuil au del√† duquel mes requ√™tes sont automatiquement rejet√©e et donc je ne paye pas plus. Sur Open AI, la facturation ne se fait pas √† l'appel (de l'API) mais au nombre de tokens. Une fois tout cela mis en place, j'ai pu g√©n√©rer et r√©cup√©rer ma cl√© API. 
J'ai √©galement d√ª installer le package `openai` en utilisant la commande `pip install openai`.
L'API d'Open AI permet d'utiliser de nombreux mod√®les dont par exemple gpt-3.5-turbo ou encore text-davinci-003.

### 3.3.3 Le mod√®le utilis√©

Pour r√©pondre √† mes besoins, j'ai d√©cid√© d'utiliser le mod√®le de completions `text-davinci-003`. Ce mod√®le est cens√© √™tre le plus performant parmi ceux disponibles pour les t√¢ches de linguistique. En effet, il doit pouvoir effectuer n'importe quelle t√¢che de linguistique comme la traduction ou la correction orthographique en apportant une qualit√© sup√©rieure et un bon suivi des instructions. 
`text-davinci-003` a une limite maximale de 4 097 tokens. Les tokens sont des fragments de texte qui peuvent √™tre aussi courts qu'un seul caract√®re ou aussi longs qu'un mot. La limite de tokens est importante car les mod√®les ont une contrainte sur le nombre maximum de tokens qu'ils peuvent traiter dans une seule entr√©e ou sortie.

### 3.3.4 Le prompt

Comme pour chatGPT, j'ai d√ª cr√©er un prompt pour mon mod√®le. Le prompt est une phrase qui va permettre au mod√®le de comprendre ce que l'on attend de lui. Ainsi, j'ai cr√©√© un prompt qui permettra √† mon mod√®le de corriger les erreurs orthographiques et grammaticales de l'OCR. Etant donn√© que certaines sorties seront en anglais et d'autres en fran√ßais, j'ai cr√©e un prompt pour chaque langue. Voici les prompt que j'ai utilis√© : 
- Pour le fran√ßais : ```"Corrige seulement la grammaire et l'orthographe du texte suivant :\n\n {sentence}"```
- Pour l'anglais : ```"Correct only the grammar and the orthograph of the following text:\n\n {sentence}"```


## 3.4 Le back-end

### 3.4.1 L'API

Afin de pouvoir faire communiquer ensemble les diff√©rents composants de mon application j'ai d√©cid√© de cr√©er une API. Pour r√©aliser cette API j'ai utilis√© le framework `fastapi`. Ce package permet de cr√©er une API en Python. J'ai √©galement utilis√© le package `uvicorn` qui permet de lancer l'API. J'ai utilis√© la commande `pip install fastapi` pour installer `fastapi` et la commande `pip install uvicorn` pour installer `uvicorn`.

L'API en elle m√™me est constitu√©e d'un fichier main.py et d'un fichier service.py. 
Les fichiers models.py, config.py, database.db et alerting.py sont des fichiers qui permettent de faire fonctionner l'API et qui appartiennt au back-end mais qui ne sont pas des composants directs de l'API.
L'API communique √©galement avec le dossier models qui contient le mod√®le d'OCR. En effet, bien que le mod√®le ait √©t√© stock√© sur le hub de Hugging Face, il n'est pas possible de l'utiliser directement depuis le hub. Il faut donc le t√©l√©charger et le stocker dans un dossier.

Le fichier main.py est le point d'entr√©e principal de l'API. Il contient les routes principales de l'API, telles que la racine ("/"), la v√©rification de l'√©tat de sant√© de l'API ("/healthcheck"), l'inf√©rence ("/inference"), le stockage des images sur Cloudinary ("/img2cloud"), etc. Chaque route est associ√©e √† une fonction qui d√©finit son comportement.

Le fichier service.py contient la classe Service, qui fournit les fonctionnalit√©s principales de l'API. Cette classe utilise divers modules et mod√®les pour effectuer des op√©rations telles que l'OCR sur une image, la correction de texte via Open AI, la traduction, etc. On y retrouve √©galement les fonctions permettant de r√©aliser l'inf√©rence telles que : do_ocr, do_correct_text, do_translation, etc.

Les fichiers models.py, config.py et database.db sont utilis√©s pour d√©finir et g√©rer les mod√®les de donn√©es et la base de donn√©es utilis√©s par l'API. 

Le fichier config.py contient une classe Settings qui stocke les configurations du projet. Les attributs de cette classe incluent des cl√©s d'API, les informations de connexion sur Cloudinary, une URL de webhook Discord, les chemins des mod√®les de traduction sur le hub d'Hugging Face, ainsi que d'autres param√®tres. Les valeurs de configuration sont charg√©es √† partir d'un fichier .env en utilisant la biblioth√®que dotenv. Les valeurs peuvent √™tre acc√©d√©es via l'instance settings de la classe Settings.

Le fichier models.py d√©finit les mod√®les de donn√©es utilis√©s dans le projet. Trois classes sont d√©finies : User, Link et Data. Ces classes correspondent aux tables de la base de donn√©es. Chaque classe utilise la biblioth√®que SQLmodel pour d√©finir les attributs et les relations entre les tables. Par exemple, la classe Link contient des attributs tels que id, url, description, timestamp et user_id. Le fichier cr√©e √©galement une instance de moteur de base de donn√©es engine qui utilise SQLite comme backend.

Le fichier database.db est la base de donn√©es SQLite utilis√©e par l'API. 

Le fichier alerting.pycontient une fonction send_discord_notification qui est utilis√©e pour envoyer des notifications sur serveur Discord d√©di√© √† Dishdecoder. La fonction utilise la biblioth√®que discord et utilise l'URL d'un webhook Discord d√©fini dans le fichier config.py. Lorsqu'elle est appel√©e, la fonction envoie un message √† travers le webhook. La fonction est utilis√©e pour envoyer des notifications pour des √©v√©nements importants, tels que la cr√©ation d'un nouvel utilisateur ou l'√©chec d'une requ√™te.

### 3.4.2 La sch√©matisation de l'API

INSERER ICI UN SCHEMA DE L'API

### 3.4.3 La base de donn√©es relationnelle

La base de donn√©es utilis√©e dans ce projet est une base de donn√©es relationnelle, qui stocke les informations relatives aux utilisateurs et √† ce qu'ils cr√©ent sur l'application. La base de donn√©es est mise en ≈ìuvre √† l'aide du syst√®me de gestion de base de donn√©es SQLite.

La structure de la base de donn√©es est d√©finie √† l'aide de la biblioth√®que SQLmodel, qui permet de cr√©er des mod√®les de donn√©es Python qui sont ensuite mapp√©s aux tables de la base de donn√©es. Dans ce projet, nous utilisons trois tables principales : User, Link et Data.

La table User contient les informations relatives aux utilisateurs de l'application. Chaque utilisateur est repr√©sent√© par un enregistrement dans cette table, avec des attributs tels que id, username, email et password. L'attribut id est une cl√© primaire qui permet d'identifier de mani√®re unique chaque utilisateur.

La table Link stocke les liens URL des images g√©n√©r√©es par l'utilisateur et stock√©es sur Cloudinary. Chaque enregistrement dans cette table est li√© √† un utilisateur sp√©cifique √† l'aide de la cl√© √©trang√®re user_id. Les autres attributs de cette table comprennent id, url, description et timestamp, qui enregistre la date et l'heure de cr√©ation de chaque enregistrement.

La table Data est utilis√©e pour stocker les donn√©es g√©n√©r√©es par les utilisateurs dans un souci d'am√©lioration de l'application. Chaque enregistrement dans cette table est associ√© √† un utilisateur et contient des attributs tels que id, ocr_entry, corrected_ocr, translation_output et user_rating. L'attribut user_id √©tablit la relation avec la table User via une cl√© √©trang√®re.

L'utilisation d'une base de donn√©es relationnelle permet de stocker de mani√®re organis√©e et structur√©e les informations n√©cessaires au bon fonctionnement de l'application. Les relations entre les tables garantissent l'int√©grit√© des donn√©es et permettent de r√©cup√©rer facilement les informations li√©es √† un utilisateur sp√©cifique.

La cr√©ation et la gestion de la base de donn√©es sont g√©r√©es par l'instance du moteur de base de donn√©es engine cr√©√©e √† l'aide de SQLmodel. Lorsque l'application d√©marre, les tables sont cr√©√©es automatiquement si elles n'existent pas d√©j√†.

L'API contient plusieurs endpoints qui permettent d'agir sur la base de donn√©es soit en ins√©rant des donn√©es, soit en r√©cup√©rant des donn√©es.
Tous les endpoints li√©s √† la base de donn√©es se situent dans le fichier main.py. Les endpoints sont les suivants :

- /create_user : Cr√©e un nouvel utilisateur en enregistrant les informations fournies (nom d'utilisateur, adresse e-mail, mot de passe) dans la table "User" de la base de donn√©es.

- /check_user : V√©rifie les identifiants d'un utilisateur en comparant le nom d'utilisateur et le mot de passe fournis avec ceux enregistr√©s dans la base de donn√©es.

- /check_username : V√©rifie si un nom d'utilisateur est d√©j√† utilis√© en comparant le nom d'utilisateur fourni avec ceux enregistr√©s dans la base de donn√©es. Les noms d'utilisateurs doivent √™tre uniques, sans tenir compte de la casse.

- /check_email : V√©rifie si une adresse e-mail est au bon format en utilisant une expression r√©guli√®re.

- /check_password : V√©rifie si un mot de passe est au bon format en utilisant une expression r√©guli√®re.

- /get_user_id : R√©cup√®re l'ID d'un utilisateur en v√©rifiant les identifiants (nom d'utilisateur et mot de passe) fournis.

- /add_to_links : Ajoute un lien √† la liste des liens en enregistrant l'URL, la description et l'ID de l'utilisateur dans la table "Link" de la base de donn√©es.

- /get_links : R√©cup√®re la liste des liens d'un utilisateur sp√©cifique en utilisant son ID.

- /add_rating : Ajoute une note sur la qualit√© de la traduction √† la base de donn√©es en enregistrant l'ID de l'utilisateur, la note donn√©e par l'utilisateur, le texte original, le texte corrig√© et le texte traduit dans la table "Data".

- /get_data : R√©cup√®re la liste de toutes les donn√©es enregistr√©es dans table "Data".

A propos des requ√™tes utilis√©es pour interagir avec la base de donn√©es, La m√©thode query est utilis√©e pour effectuer des requ√™tes de recherche, la m√©thode add pour ajouter de nouveaux enregistrements, et la m√©thode commit pour valider les changements effectu√©s dans la session et les sauvegarder dans la base de donn√©es. 

INSERER ICI UN SCHEMA DE LA BASE DE DONNEES

### 3.4.5 Le stockage des images des utilisateurs

Afin de pouvoir stocker les images g√©n√©r√©es par les utilisateurs, j'ai choisi d'utiliser Cloudinary. Cloudinary est un service de stockage d'images bas√© sur le cloud. La version gratuite de Cloudinary permet de stocker jusqu'√† 25 Go de donn√©es. Cloudinary s'int√®gre tr√®s bien dans Dishdecoder, car il existe une biblioth√®que Python qui permet d'interagir avec l'API de Cloudinary. Cela permet d'envoyer une image sur Cloudinary, de l'y stocker et de r√©cup√©rer son URL en quelques lignes de code.
Ainsi, lorsqu'un utilisateur g√©n√®re une image de recette traduite, il a la possibilit√© de lui donner une description et de la sauvegarder dans son espace personnel. Cela va en r√©alit√© envoyer l'image sur Cloudinary et enregistrer l'URL de l'image et la description donn√©e par l'utilisateur dans la base de donn√©es. Par la suite lorsque l'utilisateur ira dans son espace personnel, il y retrouvera l'image, la description de celle-ci, ainsi que la date et l'heure √† laquelle elle a √©t√© enregistr√©e. Autrement dit, l'image n'est pas stock√©e dans la base de donn√©es, mais sur Cloudinary. Cela permet de ne pas surcharger la base de donn√©es avec des images, et de ne pas avoir √† g√©rer le stockage de ces images.


## 3.5 Le front-end

### 3.5.1 L'utilisation de Streamlit

Pour le front-end de l'application, j'ai choisi d'utiliser Streamlit. Streamlit est un framework open-source qui permet de cr√©er des applications web en Python. Streamlit poss√®de de tr√®s nombreuses fonctionnalit√©s qui permettent de cr√©er des applications web interactives et personnalis√©es. Il est notamment possible d'ajouter des formulaires, des zones de texte, des boutons, de cr√©er plusieurs pages, d'afficher des tableaux etc. La communaut√© de Streamlit √©tant tr√®s active de nouvelles fonctionnalit√©s sont ajout√©s r√©guli√®rement et il est √©galement possible de trouver des modules cr√©√©s par des utilisateurs qui permettent d'ajouter davantage de fonctionnalit√©s suppl√©mentaires. 

Streamlit ne doit cependant pas √™tre utilis√© pour le d√©ploiement d'applications ou de site web. Streamlit est avant tout un d√©monstrateur. Streamlit est donc bien adapt√© dans le cadre de ce projet.

### 3.5.2 La sch√©matisation du front-end

INSERER ICI LE SCHEMA DU FRONT-END

### 3.5.3 Les fonctionnalit√©s de l'application

L'application est compos√©e de 3 pages: la page d'accueil, la page de traduction et d'espace personnel, et la page de l'administrateur.

La page d'acceuil comprend une courte description de l'application, une image de l'√©g√©rie de l'application, ainsi que des boutons pour cr√©er un compte utilisateur ou se connecter √† un compte existant.

La seconde page comprend deux onglets. Le premier onglet affiche l'historique des enregistrements de l'utilisateur, ceux-ci sont consign√©s dans un tableau. Chaque entr√©e comprend, un id, l'image g√©n√©r√©e par l'utilisateur, la description que l'utilisateur a donn√© √† son image, et la date et l'heure o√π l'image a √©t√© enregistr√©e. Il y a √©galement un bouton qui permet de rafa√Æchir le tableau. Dans l'autre onglet, il est possible de t√©l√©charger une image, de la recadrer de fa√ßon √† n'avoir qu'un seul paragraphe, et enfin de choisir si on veut effectuer une traduction de l'anglais vers le fran√ßais ou du fran√ßais vers l'anglais. Une fois que la traduction est effectu√©e, une nouvelle image va appara√Ætre avec le texte traduit. L'utilisateur a alors la possibilit√© de donner une description √† l'image et de l'enregistrer dans son espace personnel. L'utilisateur peut √©galement choisir de donner une note √† la traduction. Enfin, en bas de la page, il y a un bouton qui permet de retourner √† la page d'accueil.

La troisi√®me page, qui n'est accessible qu'√† l'administrateur apr√®s avoir rentr√© son nom d'utilisateur et son mot de passe dans l'espace de connexion, contient un tableau o√π sont consign√©es les traductions que l'utilisateur a choisi de noter. Chaque entr√©e comprend, un id, le texte extrait par l'OCR, le texte corrig√© par OpenAI, la traduction de ce texte et la note donn√©e par l'utilisateur. Il y a √©galement un code couleur √† l'int√©rieur du tableau en fonction de la note donn√©e par l'utilisateur afin que cela attire l'attention de l'administrateur. Enfin, en bas de la page, il y a un bouton qui permet √† l'administrateur de t√©l√©charger le tableau en CSV et un autre qui permet de retourner √† la page d'accueil.

D'autre part, des messages d'erreurs sont affich√©s lorsque l'utilisateur ne remplit pas correctement les champs des formulaires ou lorsqu'il y a un probl√®me de connexion √† l'API ou encore losrque l'API renvoie une erreur.

Lorsque l'API renvoie un message d'erreur ou lorsqu'un nouveau utilisateur est cr√©e, un message est envoy√© √† l'administrateur sur le serveur Discord de Dishdecoder en lui sp√©cifiant le type d'erreur ou le nom du nouvel utilisateur. Cela permet √† l'administrateur de pouvoir r√©agir rapidement en cas de probl√®me.

Enfin, l'int√©r√™t d'afficher un tableau avec les notes des utilisateurs √† propos des traductions est de pouvoir am√©liorer les mod√®les d'intelligence artificielle. En effet, l'administrateur peut voir quelles sont les traductions qui ont √©t√© mal not√©es et en tenir compte lorsqu'il constituera un nouveau jeu de donn√©es pour r√©-entra√Æner les mod√®les. Cela permettra d'am√©liorer les mod√®les et donc d'am√©liorer la qualit√© des traductions. 

### 3.5.4 L'interaction avec le back-end

L'interaction avec la base de donn√©es relationnelle et avec les mod√®les d'intelligence artificelle se fait gr√¢ce √† des appels API. Pour cela, j'ai utilis√© la biblioth√®que Python requests. Cette biblioth√®que permet d'envoyer des requ√™tes HTTP. Ainsi, pour envoyer une requ√™te √† l'API, il suffit de pr√©ciser l'URL de l'API, le type de requ√™te (GET, POST, PUT, DELETE) et les param√®tres de la requ√™te. Afin d'int√©grer ces appels √† l'API convenablement dans l'application, j'ai cr√©√© une fonction pour chaque requ√™te. Ainsi, lorsque l'utilisateur clique sur un bouton, la fonction correspondante est appel√©e et la requ√™te est envoy√©e √† l'API.
Cela permet √©galement de bien s√©parer le front-end du back-end. En effet, le front-end sert uniquement √† l'affichage et ne fait que des appels √† l'API, il n'interagit pas directement avec la base de donn√©es ou avec les mod√®les d'intelligence artificielle. Cela permet de rendre l'application plus robuste et plus facile √† maintenir. En effet, si l'on souhaite modifier la base de donn√©es ou les mod√®les d'intelligence artificielle, il suffit de modifier l'API, sans avoir √† modifier le front-end.

## 3.6 L'organisation du projet

### 3.6.1 Le GANTT

### 3.6.2 Le Kanban

### 3.6.3 Le GitHub

D√®s le d√©but du projet, j'ai cr√©√© un d√©p√¥t GitHub pour pouvoir versionner mon code. Ainsi, √† chaque ajout de fonctionnalit√©, je faisais un commit depuis mon IDE, VSCode. Cela √©tait rassurant car je gardaus la possibilit√© de revenir √† une version ant√©rieure de mon code en cas de probl√®me. Aussi, Cela m'a √©t√© utile pour pouvoir travailler sur diff√©rents ordinateurs sans avoir √† transf√©rer les fichiers. En effet, il suffisait de cloner le d√©p√¥t sur l'ordinateur sur lequel je souhaitais travailler. 

# IV. CONCLUSION

## 4.1 La r√©alisation du projet

R√©aliser Dishdecoder fut une exp√©rience enrichissante pour de nombreuses raisons. 
Tout d'abord, en raison de la partie sur la traduction. Venant d'une formation sp√©cialis√©e dans les langues √©trang√®res, j'ai toujours √©t√© int√©ress√©e et passion√©e par la traduction. Cependant, je n'avais pas encore eu l'occasion de m√™ler mes nouvelles connaissances en intelligence artificielle √† ma passion pour la traduction. C'est donc avec beaucoup d'enthousiasme et d'int√©r√™t que j'ai r√©alis√© ce projet et d√©couvert cette branche de la NLP. 
Ensuite, je n'avais pas encore eu l'opportunit√© de travailler sur un projet qui utilise plusieurs mod√®les d'intelligence artificielle √† la suite. J'ai donc appr√©ci√© pouvoir combiner ici un mod√®le d'OCR, un mod√®le de correction de texte et un mod√®le de traduction. 
Par ailleurs, ce projet m'a prouv√© une fois encore que ce qui me pla√Æt le plus est la constitution de jeux de donn√©es. En effet, j'ai pass√© beaucoup de temps √† r√©unir les donn√©es que je souhaitais pour le mod√®le d'OCR et les mod√®les de traduction, mais cela n'a jamsi √©t√© un fardeau car j'ai pu obtenir exactement ce que je voulais et en plus de cela, les r√©sultats ont √©t√© tr√®s satisfaisants.
Enfin, avoir un projet que j'ai cr√©e de toute pi√®ce et qui fonctionne est tr√®s gratifiant. Cela m'a permis de voir que j'√©tais capable de r√©aliser un projet de A √† Z et de le mener √† bien.

## 4.2 Les difficult√©s rencontr√©es

Je pense que la difficult√© majeure dans ce genre de projet est de r√©ussir √† s'arr√™ter. Que ce soit lors de la constitution de jeux de donn√©es, lors de l'entra√Æenment de mod√®le ou lors de la r√©alisation de l'interface graphique et de l'int√©gration des diff√©rentes fonctionnalit√©s, il m'a toujours √©t√© tr√®s compliqu√© de savoir quand m'arr√™ter. En effet, il y a toujours quelque chose √† am√©liorer, √† modifier ou √† ajouter. Cependant, il faut savoir s'arr√™ter √† un moment donn√© pour pouvoir passer √† autre chose.
Une autre difficult√© fut dans le choix de la technologie √† utiliser pour l'interface graphique. En effet, lors de ma formation et de mon alternance nous avons tr√®s souvent utilis√© Streamlit. J'appr√©cie beaucoup Streamlit car je trouve leur documentation tr√®s claire et en plus il y a toujours des nouveaut√©s qui apportent de r√©els changements. Cependant, j'avais envie de changer et d'utiliser une autre technologie. J'ai donc commenc√© √† utiliser Pynecone. La prise en main fut un peu plus compliqu√©e car la documentation est moins claire et surtout il y a beaucoup moins de ressources sur internet √©tant donn√© que Pynecone est relativement r√©cent. Toutefois, j'avais r√©ussi √† cr√©er ma premi√®re page. Les probl√®mes vinrent lorsque j'ai eu besoin d'utiliser un wrapper React afin d'utiliser un widget me permettant de recadrer les images. Malgr√© tous mes efforts cela n'a jamais fonctionn√© comme je l'esp√®rais et lorsque j'ai voulu int√©grer FastAPI √† Pynecone j'ai compris que cela n'allait pas √™tre possible non plus. J'ai donc d√©cid√© de revenir √† Streamlit. Cela m'a permis de me rendre compte que Streamlit est vraiment une technologie que j'appr√©cie et que je ma√Ætrise bien.

## 4.3 Les am√©liorations et les perspectives d'√©volution

J'ai d√©j√† de nombreuses id√©es pour am√©liorer Dishdecoder. 
Parmi les choses simples √† faire, il y a :
- le fait d'int√©grer un bouton qui permet √† l'utilisateur de t√©l√©charger sur son ordinateur l'image qu'il a g√©n√©r√©e.
- le fait d'int√©grer √† l'interface administateur des statistiques sur les notes donn√©es aux traductions par les utilisateurs.

Parmi les choses qui prennent un peu plus de temps, il y a :
- le fait d'int√©grer de nouvelles langues de traduction, notamment l'espagnol et l'italien.
- le fait de finetuner un mod√®le me permettant de faire de la correction orthographique.
- le fait d'augmenter le nombre d'entr√©es dans le jeu de donn√©es pour les mod√®les de traduction.
- trouver un moyen pour me passer du recadrage des images
- trouver un moyen de convertir les unit√©s de mesures dans les recettes

Je pense que Dishdecoder est une application qui peut avoir de l'avenir et qui saurait sans doute trouver son public. Certes TextGrabber et Deepl existent mais il para√Æt que l'humain pr√©f√®re de plus en plus aller √† la simplicit√©. Alors quoi de plus simple que de prendre une photo de son livre de recettes et de se faire traduire la recette en un clic ? 