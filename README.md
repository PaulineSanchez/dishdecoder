# dishdecoder
üçΩÔ∏è

### RAPPORT SUR LE PROJET DISHDECODER

PLAN :

I. Introduction

    1.1 Contexte
    1.2 Objectifs
    1.3 Approche de la solution
    1.4 Technologies utilis√©es
    1.5 Sch√©ma de l‚Äôapplication

II. ETAT DE L‚ÄôART

III. DISHDECODER

    3.1 Le mod√®le d'OCR
        3.1.1 Le choix du mod√®le
        3.1.2 Les donn√©es n√©cessaires √† l‚Äôentra√Ænement du mod√®le
        3.1.3 La cr√©ation du dataset
        3.1.4 Le pr√©traitement des donn√©es
        3.1.5 Le stockage du dataset
        3.1.6 L‚Äôentra√Ænement du mod√®le
        3.1.7 Configuration des hyperparam√®tres du mod√®le
        3.1.8 Comparaison des mod√®les et choix du meilleur
        3.1.9 Le stockage du mod√®le

    3.2 Les mod√®les de traduction
        3.2.1 Le choix des mod√®les
        3.2.2 Les donn√©es n√©cessaires √† l‚Äôentra√Ænement des mod√®les
        3.2.3 La cr√©ation du dataset
        3.2.4 Le pr√©traitement des donn√©es
        3.2.5 Le stockage du dataset
        3.2.6 L‚Äôentra√Ænement des mod√®les
        3.2.7 Configuration des hyperparam√®tres des mod√®les
        3.2.8 Comparaison des mod√®les et choix des meilleurs
        3.2.9 Le stockage des mod√®les

    3.3 L'utilisation d'Open AI
        3.3.1 Pourquoi le choix d'Open AI ?
        3.3.2 L'API d'Open AI
        3.3.3 Le mod√®le utilis√©
        3.3.4 Le prompt

    3.4 Le back-end
        3.4.1 L‚ÄôAPI
        3.4.2 La sch√©matisation de l‚ÄôAPI
        3.4.3 La base de donn√©es relationnelle
        3.4.4 Le stockage des images des utilisateurs

    3.5 Le front-end
        3.5.1 L'utilisation de Streamlit
        3.5.2 La sch√©matisation du front-end
        3.5.3 Les fonctionnalit√©s de l'application
        3.5.4 Le parcours utilisateur
        3.5.5 Le parcours administrateur
    
    3.6 L'organisation du projet
        3.6.1 Le Trello (GANT ?)
        3.6.2 Le Kanban
        3.6.2 Le GitHub

IV. CONCLUSION

    4.1 La r√©alisation du projet
    4.2 Les difficult√©s rencontr√©es
    4.3 Les am√©liorations possibles
    4.4 Les perspectives d'√©volution
    

# I. Introduction

## 1.1. Contexte

Depuis plusieurs mois maintenant, l'Intelligence Artificielle est au coeur de l'actualit√©. Tant√¥t adul√©e, par certains qui voient en elle un moyen de r√©soudre les probl√®mes de l'humanit√©, tant√¥t d√©cri√©e, par d'autres qui la voient comme une menace pour l'emploi et la survie de l'esp√®ce, l'Intelligence Artificielle est un sujet qui fait d√©bat. 
C'est dans ce contexte que je me suis mise en qu√™te d'un projet √† r√©aliser o√π je pourrais m√™ler √† la fois mes connaissances en IA et un sujet qui est tr√®s terre √† terre : la nourriture. Les premi√®res id√©es qui me sont venues concernaient la lecture des ingr√©dients pr√©sents sur les emballages alimentaires et la d√©tection de substances dangereuses dans certains plats pr√©par√©s, cependant de nombreuses applications exitent d√©j√† sur le march√© et je ne voyais pas comment je pourrais apporter une plus-value √† ce qui existe d√©j√†.
C'est au d√©tour d'une conversation avec mon p√®re que l'id√©e de Dishdecoder m'est venue. En effet, celui-ci me demandait si je connaissais une application o√π lorsqu'on lui envoie une photo avec du texte celle-ci est capable de renvoyer une traduction de ce texte dans la langue souhait√©e. Bien sur, il existe d√©j√† des applications qui extraient du texte pr√©sent sur une image, des applications faisant de la traduction, et m√™me des applications faisant les deux comme par exemple TextGrabber, mais je me suis dit que je pourrais essayer de faire quelque chose de similaire, sp√©cialis√© dans la cuisine et plus pr√©cis√©ment dans la lecture et la traduction de recettes de cuisine. C'est ainsi que Dishdecoder est n√©.

## 1.2. Objectifs

L'objectif de Dishdecoder est d'√™tre une application permettant √† un utilisateur de prendre en photo une recette de cuisine et d'obtenir une traduction de celle-ci dans la langue de son choix. L'application doit √™tre capable de d√©tecter le texte pr√©sent sur l'image, de le reconnaitre et de le traduire. Dans un premier temps, l'application devra √™tre capable de fournir des traductions de recettes de cuisine en anglais et en fran√ßais. L'application devra √©galement proposer √† l'utilisateur de sauvegarder les recettes traduites dans un espace personnel. Enfin, l'application devra permettre √† l'administrateur de voir les traductions r√©alis√©es afin de pouvoir r√©colter des donn√©es pour am√©liorer les mod√®les de traduction.

## 1.3 Approche de la solution

Pour r√©aliser ce projet, plusieurs composants allaient √™tre n√©cessaires:
- Les mod√®les d'intelligence artificielle : celui d'extraction de texte √† partir d'image et ceux de traduction
- Les bases de donn√©es : celles contenant les datasets pour entrainer les mod√®les et celle contenant les donn√©es de l'application
- La mise en place du back-end de l'application : l√† o√π les mod√®les communiqueront avec l'API
- La r√©alisation du front-end de l'application : l√† o√π l'utilisateur pourra interagir avec l'application qui elle m√™me inter√©agira avec l'API

## 1.4 Technologies utilis√©es

Pour r√©aliser ce projet, j'ai utilis√© les technologies suivantes :
- Python : comme langage de programmation
- Pytorch : pour la r√©alisation des mod√®les d'intelligence artificielle
- FastAPI : pour la r√©alisation de l'API
- SQLModel : pour la r√©alisation de la base de donn√©es relationnelle
- Streamlit : pour la r√©alisation du front-end de l'application
- HuggingFace Hub : pour le stockage des mod√®les d'intelligence artificielle et des datasets
- Cloudinary : pour le stockage d'images
- PadddleOCR : pour la r√©alisation du mod√®le d'extraction de texte √† partir d'image
- HuggingFace : pour la r√©alisation du mod√®le de traduction
- OpenAI : pour l'utilisation de leur API 
- Playground AI : pour la r√©alisation de l'√©g√©rie de l'application


## 1.5 Sch√©ma de l'application

INSERER ICI LE SCH√âMA DE L'APPLICATION

# II. ETAT DE L'ART

INSERER ICI L'ETAT DE L'ART

# III. DISHDECODER

## 3.1 Le mod√®le d'OCR

### 3.1.1 Le choix du mod√®le

Comme nous l'avons vu pr√©c√©demment, il existe √† l'heure actuelle de tr√®s nombreux mod√®les pour r√©aliser de l'OCR (Optical Character Recognition).

Lors de mon alternance j'ai √©t√© en charge de la r√©alisation d'un mod√®le d'OCR pour une application de lecture de documents. J'ai donc eu l'occasion de tester plusieurs mod√®les d'OCR et de voir leurs avantages et leurs inconv√©nients. Parmi les mod√®les que j'ai test√©, il y avait Tesseract, EasyOCR, TROCR et PaddleOCR. 

Tesseract, qui est sans doute le plus connu, a √©t√© d√©velopp√© √† l'origine par Hewlett-Packard Labs et d√©sormais maintenu par Google. Tesseract est un mod√®le open source et est de base fait pour √™tre utilis√© en language C++ mais il existe des wrappers pour l'utiliser en Python, comme par exemple Pytesseract. Cependant, apr√®s test, Tesseract n'est pas le mod√®le le plus performant, notamment sur les images avec du bruit. De plus, il n'est pas toujours simple √† configurer selon l'utilisation que l'on souhaite en faire et aussi son impl√©mentation en Python peut se r√©veler tr√®s compliqu√©e en raison d'incompatibilit√©s avec d'autres librairies. C'est pourquoi, lors de mon alternance, j'ai d√©cid√© de ne pas utiliser Tesseract.

EasyOCR est quant √† lui connu pour √™tre une librairie open-source, d√©velop√©e par JaidedAi, qui prend en charge plus de 70 langues et qui est compatible avec de nombreux langages de programmation dont Python, C#, Java. En plus de cela, EasyOCR est tr√®s rapide. Par ailleurs, comme son nom l‚Äôindique EasyOCR est facile √† utiliser. Il suffit de trois lignes, dont l'importation, pour qu‚Äôil lise une image. Cependant, j'avais remarqu√© que les sorties de celui-ci n‚Äô√©taient pas toujours exactes. Que ce soit pour la lecture des chiffres pr√©sents sur les documents ou la lecture des diff√©rentes zones de texte, de nombreuses erreurs √©taient r√©alis√©es √† chaque lecture : confusion entre deux caract√®res, oubli d‚Äôespace, ajout de caract√®res etc. Malgr√© divers r√©glages dans les param√®tres, les r√©sultats restaient toujours brouillons.

TROCR (Tokenization, Recognizing, and OCR) est √©galement une librairie open-source, datant de 2017, d√©velopp√©e par Microsoft et impl√©ment√©e
dans Transformers, la librairie open-source d‚ÄôHugging Face. TROCR combine les √©tapes d‚ÄôOCR pour extraire le texte des images, et de tokenisation pour convertir le texte en une s√©quence de tokens utilisable par des mod√®les de traitement du langage naturel. Contrairement √† EasyOCR, le traitement des images via TROCR est beaucoup plus long. Malgr√© cela j'ai quand m√™me test√© TROCR, avec le mod√®le base-printed qui est cens√© √™tre le mod√®le le plus adapt√© √† la lecture de caract√®res d‚Äôimprimerie. En effet, le mod√®le base-printed a √©t√© fine-tun√© sur le dataset SROIE qui est constitu√© d‚Äôun millier de tickets de caisse. Apr√®s de nombreux tests, les r√©sultats obtenus avec TROCR √©taient g√©n√©ralement meilleurs que ceux obtenus avec EasyOCR. Toutefois, TROCR donnait exclusivement des sorties en majuscules, la ponctuation √©tait approximative, les espaces pas toujours respect√©s et surtout, le temps d‚Äôinf√©rence du mod√®le √©tait beaucoup trop important. TROCR doit √™tre r√©serv√© √† un usage sur GPU, ce qui n'√©tait pas possible lors de mon altenance et qui ne l'est toujours pas aujourd'hui avec Dishdecoder.

PaddleOCR est, comme pour les trois OCR pr√©c√©dents, une librairie open-source. Celle-ci est d√©velopp√©e par PaddlePaddle (PArallel Distributed Deep LEarning), une entreprise chinoise. Elle utilise des algorithmes de pointe pour d√©tecter, localiser et reconna√Ætre des caract√®res dans des images, et est disponible dans de
nombreuses langues. L‚Äôutilisation de PaddleOCR ressemble √† celle d‚ÄôEasyOCR par sa facilit√©. L√† aussi, la librairie est compatible avec plusieurs langages de programmation tels que Python, C++ et Java, ce qui facilite son int√©gration dans diff√©rents projets.
L‚Äôarchitecture de deep-learning utilis√©e derri√®re PaddleOCR est un CRNN : Convolutional Recurrent Neural Network. Le CRNN est un r√©seau de neurones profonds qui combine √† la fois un CNN (Convolutional Neural Network) et un RNN (Recurrent Neural Network). 
Dans un CRNN, la couche de convolution, CNN, est utilis√©e pour extraire des caract√©ristiques pertinentes des images. Ensuite, la couche RNN est utilis√©e pour analyser et traiter ces caract√©ristiques dans un contexte de s√©quence. Les sorties de la couche de convolution sont
transform√©es en s√©quences d'entr√©e pour le RNN, qui utilise sa m√©moire interne pour prendre en compte les caract√©ristiques pr√©c√©dentes tout en analysant la s√©quence actuelle. Cela permet au CRNN de mieux comprendre la structure de l'image et d'extraire des informations textuelles
pertinentes.
Concernant PaddlePaddle, il s‚Äôagit de la premi√®re plateforme open-source ind√©pendante de deep learning en Chine. Elle est d√©velopp√©e par l‚Äôentreprise Baidu. PaddlePaddle est plus ou moins l‚Äô√©quivalent chinois de TensorFlow. La plateforme PaddlePaddle est r√©put√©e pour sa rapidit√© d‚Äôinf√©rence, sa capacit√© √† traiter des volumes de donn√©es massifs et son large √©cosyst√®me de modules et d'outils int√©gr√©s. La plateforme est largement utilis√©e dans diff√©rents domaines, notamment l'industrie, le transport, l‚Äôagriculture et les services publics. 
D√®s la premi√®re utilisation de PaddleOCR, avec le mod√®le de base en langue fran√ßaise et latine, j‚Äôai  pu observer que les sorties donn√©es √©taient bien meilleures que celles d‚ÄôEasyOCR. Certains points √©taient malgr√© tout perfectibles : les espaces entre les caract√®res n‚Äô√©taient souvent pas retranscrits et il y avait parfois de la confusion entre certains caract√®res. Cependant, il √©tait mis en avant sur le GitHub de PaddleOCR qu'avec du finetuning, les r√©sultats pouvaient √™tre nettement am√©lior√©s. 

Voici un exemple de test que j'ai r√©alis√© lors de mon alternance sur EasyOCR, TROCR et PaddleOCR :

INSERER ICI LES IMAGES MR OU MME BERTIN


Comme nous pouvons l'observer, les r√©sultats obtenus par TROCR et PaddleOCR sont bien concluants que ceux obtenus par EasyOCR. 

C'est donc pour ces diverses raisons que lors de mon alternance j'ai d√©cid√© de finetuner PaddleOCR. Les bons r√©sultats obtenus font que mon mod√®le finetun√© est d√©sormais le mod√®le utilis√© par mon entreprise d'alternance lorsqu'il s'agit de faire de l'OCR.

Ne pouvant r√©utiliser le mod√®le et le dataset d'entra√Ænement de mon alternance pour Dishdecoder, j'ai choisi de reconstituer un nouveau dataset d'entra√Ænement et de finetuner un mod√®le de PaddleOCR sur ce nouveau dataset.

### 3.1.2 Les donn√©es n√©cessaires √† l'entra√Ænement du mod√®le

Avant de savoir quelles donn√©es sont n√©cessaires pour finetuner un mod√®le de PaddleOCR, il est important de comprendre comment fonctionne PaddleOCR.
En effet, il n'existe pas qu'un seul mod√®le PaddleOCR qui sait g√©rer toutes les langues. Il existe un mod√®le par langue. Par exemple, il existe un mod√®le pour la langue fran√ßaise, un mod√®le pour la langue anglaise, un mod√®le pour la langue chinoise etc. Il y a √©galement plusieurs versions, il y a par exemple les mod√®le PP-OCRv2 de l'ancienne g√©n√©ration et les PP-OCRv3 qui sont les plus r√©cents et les plus performants. A vitesse √©gale, selon les langues, les mod√®les PP-OCRv3 sont entre 5% et 11% plus pr√©cis (accuracy) que les mod√®les PP-OCRv2.
Par ailleurs, PaddleOCR propose d‚Äôutiliser trois types de mod√®les diff√©rents qui peuvent √™tre combin√©s lors de la lecture d‚Äôune image. Il y a un mod√®le de reconnaissance de caract√®res (recognition model aussi appel√© rec), un mod√®le de d√©tection de texte (detection model aussi appel√© det) et un mod√®le de classification d‚Äôangle du texte (classification model aussi appel√© cls). Dans mon cas d‚Äôusage, je n‚Äôavais pas besoin de finetuner mon mod√®le pour qu‚Äôil soit plus performant dans la d√©tection de texte ou dans la reconnaissance de l‚Äôorientation du texte. Je souhaitais uniquement avoir un mod√®le plus performant sur la reconnaissance de caract√®res, c‚Äô√©tait donc un recognition model que j‚Äôallais finetuner. Par ailleurs, plut√¥t que de finetuner le mod√®le de reconnaissance fran√ßais, french_mobile_v2.0_rec, j‚Äôai choisi de finetuner le mod√®le de reconnaissance latin appel√©
latin_PP-OCRv3_rec. En effet, le mod√®le latin fait partie des mod√®les de 3√®me g√©n√©ration, ce qui implique de meilleures performances.

Il est recommand√©, sur le GitHub de PaddleOCR, d'utiliser plus de 5000 images lors du finetuning d'un mod√®le de reconnaissance. 

D'experience, je sais √©galement que plus le dataset comprend des images diverses et vari√©es et plus le mod√®le finetun√© est performant. Il √©tait donc n√©cessaires que je receuille des images avec diff√©rentes tailles, diff√©rentes polices de caract√®res, diff√©rents types de fonds, diff√©rentes couleurs, diff√©rentes longueur de texte, des majuscules, des minuscules, des chiffres, des caract√®res sp√©ciaux etc.

Souhaitant √©galement que mon OCR soit performant sur du fran√ßais comme sur de l'anglais, j'ai d√©cid√© de r√©colter des images en fran√ßais et en anglais.

Pour finetuner un mod√®le de reconnaissance PaddleOCR, le dataset doit √™tre constitu√© de la mani√®re suivante : il doit y avoit des images et un fichier texte. Le fichier texte doit contenir deux √©l√©ments pour chaque image: le chemin de sa localisation avec le nom donn√© √† l'image et le texte pr√©sent sur l'image. Le chemin de l'image et le texte pr√©sent sur l'image doivent √™tre s√©par√©s par une tabulation. 

### 3.1.3 La cr√©ation du dataset

N'ayant trouv√© aucun dataset pertinent r√©pondant √† mes besoins, j'ai d√©cid√©, comme lors de mon alternance, de cr√©er mon propre dataset.
Pour cela, aucun outil de labellisation n'est n√©cessaire. 
En effet, le seul outil dont j'avais besoin √©tait un outil de capture d'√©cran. J'ai donc utilis√© l'outil de capture d'√©cran int√©gr√© √† mon ordinateur, celui de Ubuntu.
Afin de constitu√© mon dataset, j'ai √©t√© sur de nombreux sites internet dont : 
- Amazon.com o√π j'ai recherch√© diff√©rents ouvrages en fran√ßais et en anglais. J'y ai captur√© des images de titres de livres, de quatri√®me de couverture, de table des mati√®res...
- Brest.fr o√π j'ai pu trouver certains extraits de magazines, des infographies, des affiches, des publicit√©s...
- Des sites internet d'archive o√π sont scann√©s d'anciens num√©ros de journaux actuels comme Le Monde ou Le Canard Encha√Æn√©, o√π j'ai pu capturer des images de titres d'articles etc.
- Des sites internet sp√©cialis√©s dans les journaux et annuaires anciens pour trouver des polices d'√©criture plus rares.
- Des sites internet de restaurants dont Restaurantguru.com o√π j'ai pu capturer des images √† partir des diff√©rents menus.

J'ai r√©colt√© en tout 1285 images. Pour chaque image, j'ai recens√© son chemin avec son nom ainsi que le texte pr√©sent dans cette m√™me image, le tout consign√© dans un fichier texte.

### 3.1.4 Le pr√©traitement des donn√©es

Parmi les images r√©cup√©r√©es, j'en ai s√©lectionn√© 565 pour l'entra√Ænement et 720 pour la validation.
Afin de rendre mon dataset plus robuste, j'ai fait de la data augmentation sur mon dataset d'entrainement. 
Pour cela, j'ai adapt√© le code du projet Straug pr√©sent sur le GitHub de Roatenzia √† ce dont j‚Äôavais besoin.
J‚Äôai choisi d‚Äôappliquer √† mes images les effets suivants : GaussianNoise, SpeckleNoise, ImpulseNoise, MotionBlur, DefocusBlur, Shadow, Rain. 

INSRER IMAGE EXEMPLE

Une fois les nouvelles images g√©n√©r√©es, 17 nouvelles images en sortie pour 1 image en entr√©e, soit 9605 nouvelles images, j‚Äôai pu les labelliser dans mon fichier texte d‚Äôentra√Ænement et les d√©placer dans mon dossier comprenant toutes les images.
Ainsi le dataset final pour ce finetuning comprenait 10170 images pour l‚Äôentra√Ænement et 720 images pour la validation.
L'ensemble du dataset a √©t√© mis dans un dossier train_data. Dans train_data il y a un dossier data o√π sont stock√©es toutes les images du dataset et deux fichiers .txt : dataset_IMG_train_REDO.txt et dataset_IMG_val_REDO.txt . Comme leurs noms le laisse supposer, le fichier dataset_IMG_train_REDO.txt contient les chemins ainsi que le texte contenu sur les images d'entra√Ænement et le fichier dataset_IMG_val_REDO.txt contient les chemins ainsi que le texte contenu sur les images de validation.

### 3.1.5 Le stockage du dataset

J'aurais souhait√© stocker mon dataset sur le Hub de Hugging Face, mais cela n'a pas pu √™tre fait car un repository sur HuggingFace ne peut pas contenir plus de 10000 √©l√©ments. J'ai donc stock√© mon dataset dans un dossier sur mon ordinateur. 

### 3.1.6 L'entrainement du mod√®le

La premi√®re √©tape avant de lancer l'entra√Ænement, fut de cloner le repository GitHub de PaddleOCR sur mon ordinateur.
```$ git clone https://github.com/PaddlePaddle/PaddleOCR.git```
Une fois le projet clon√©, il fallait, √† l‚Äôint√©rieur du dossier principal, cr√©er un dossier pretrain_models.
En effet, lorsque l‚Äôon finetune un mod√®le, il nous faut obligatoirement un mod√®le de base, mod√®le auquel on va apporter des modifications avec le finetuning. Pour les raisons expliqu√©es plus haut, j‚Äôavais choisi le mod√®le latin_PP-OCRv3_rec. Il fallait alors t√©l√©charger ce mod√®le depuis le GitHub de PaddleOCR et l‚Äôextraire dans le dossier pretrain_models.
Apr√®s cela, √† la racine du dossier principal PaddleOCR, au m√™me niveau que pour le dossier pretrain_models, il fallait copier le dossier train_data comprenant le dataset.
L‚Äô√©tape suivante √©tait de retourner dans le dossier principal PaddleOCR, puis d‚Äôaller dans le dossier configs, puis rec, puis PP-OCRv3, puis multi_language, puis de faire une copie du fichier latin_PPOCRv3_rec.yml en le renommant. C‚Äôest dans ce fichier de configuration qu‚Äôinterviennent les
modifications de chemins afin de permettre √† PaddleOCR d‚Äôutiliser le dataset contenu dans train_data lors de l‚Äôentra√Ænement. Ce fichier de configuration permet √©galement de sp√©cifier bon nombre de param√®tres : le chemin du dictionnaire √† utiliser, la prise en compte des espaces, mais
√©galement le chemin du mod√®le pr√©-entra√Æn√© √† utiliser etc.
Une fois toutes ces √©tapes r√©alis√©es, j'ai d√©cid√© de cr√©er un nouvel environnement conda d√©di√© √† l'entrainement de mon mod√®le. Pour cela, j'ai utilis√© la commande suivante : 
```$ conda create --name paddle-training python=3.10```
J'ai ensuite activ√© cet environnement avec la commande suivante : 
```$ conda activate paddle-training```
Apr√®s cela, afin d'utiliser la version GPU pour l'entra√Ænement du mod√®le, j'ai fait la commande suivante qui est recommand√©e sur le site de Paddlepaddle: 
```python -m pip install paddlepaddle-gpu==2.4.2.post117 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html```
Enfin j'ai install√©, en prenant bien soin d'√™tre √† ce moment-l√† dans le dossier PaddleOCR, les d√©pendances n√©cessaires √† l'entra√Ænement du mod√®le avec la commande suivante : 
```$ pip install -r requirements.txt```
Une fois toutes ces √©tapes r√©alis√©es, tout en √©tant toujours bien √† la racine du dossier PaddleOCR, j'ai pu lancer l'entra√Ænement de mon mod√®le avec la commande suivante : 
```$ python3 -m paddle.distributed.launch --gpus '0'  tools/train.py -c configs/rec/PP-OCRv3/multi_language/latin_PP-OCRv3_rec_4_finetuning.yml```

L'entra√Ænement, de 200 epochs, a √©t√© lanc√© √† 12h49 et s'est termin√© √† 23h18, soit un peu plus de 10h d'entra√Ænement.
Comme sp√©cifi√© dans le fichier de configuration, les poids du mod√®le finetun√© ayant la meilleure pr√©cision (accuracy) ainsi que ses diff√©rents fichiers (fichier de configuartaion, fichier qui repertorie tous les logs d'entra√Ænement, poids du mod√®le actualis√©s √† chaque epoch...) ont √©t√© stock√©s dans le dossier ./output/v3_latin_mobile_FINETUNING_1

Vis-√†-vis de la m√©thode d'entra√Ænement, Le finetuning est souvent consid√©r√© comme une forme d'entra√Ænement supervis√© car il n√©cessite des annotations √©tiquet√©es pour guider le processus d'apprentissage. Cependant, on peut √©galement le consid√©rer comme une approche semi-supervis√©e, car le mod√®le de base pr√©-entra√Æn√© apporte une connaissance pr√©alable supervis√©e qui est ensuite am√©lior√©e et affin√©e gr√¢ce aux donn√©es suppl√©mentaires annot√©es que nous lui fournissons. 

### 3.1.7 Configuration des hyperparam√®tres du mod√®le

Lors de ce premier finetuning, j'ai choisi de modifier les hyperparam√®tres suivants :
- le batch size, que j'ai mis √† 16
- le nombre d'epochs, que j'ai mis √† 200

Etant donn√© qu'apr√®s ce premier entra√Ænement j'ai eu l'occasion d'avoir acc√®s √† un GPU plus puissant, j'ai d√©cid√© de refaire un finetuning avec cette fois-ci un batch size de 128 et un nombre d'epochs de 200.
Ensuite, j'ai d√©cid√© de faire un troisi√®me finetuning avec un batch size de 64 et un nombre d'epochs de 200.
Puis, j'ai d√©cid√© de faire un quatri√®me finetuning avec un batch size de 64 et un nombre d'epochs de 750.
Enfin, j'ai d√©cid√© de faire un cinqui√®me finetuning avec un batch size de 32 et un nombre d'epochs de 200.

### 3.1.8 Comparaison des mod√®les et choix du meilleur 

N'ayant pas utilis√© d'outil de monitoring pour les entra√Ænements de mon mod√®le, je n'ai pas pu avoir acc√®s √† des graphiques repr√©sentant l'√©volution de l'accuracy au cours des entra√Ænements. Cependant, j'ai pu avoir acc√®s √† ces informations dans le fichier de log de chaque entra√Ænement. 
Par ailleurs, √† la fin de chaque entra√Ænement, PaddleOCR indique qu'elle fut l'epoch o√π a √©t√© calcul√©e la meilleure accuracy sur le dataset de validation. Pour PaddleOCR l'accuracy est la m√©trique utilis√©e pour √©valuer la performance du mod√®le.

Voici les r√©sultats obtenus pour chaque entra√Ænement :

INSERER ICI LES RESULTATS OBTENUS POUR CHAQUE ENTRAINEMENT

Comme nous pouvons le constater, le mod√®le ayant la meilleure accuracy est le premier √† avoir √©t√© entra√Æn√©. C'est donc celui-ci que nous allons utiliser pour la suite de ce projet.

Cependant, le mod√®le obtenu apr√®s l'entra√Ænement n'est pas utilisable en l'√©tat. Apr√®s entra√Ænement tous les mod√®les PaddleOCR sont compos√©s de trois fichiers : un fichier en .pdopt, un fichier en .pdparams et un fichier en .states. Dans notre cas ces trois fichiers sont stock√©s dans le dossier ./output/v3_latin_mobile_FINETUNING_1/best_accuracy. Afin de pouvoir utiliser ce mod√®le, il a donc fallu le convertir en un mod√®le utilisable par PaddleOCR.

 Pour cela, j'ai √©xecut√© la commande suivante : 
```$ python3 tools/export_model.py -c configs/rec/PP-OCRv3/multi_language/latin_PP-OCRv3_rec_4_finetuning.yml -o Global.pretrained_model=./output/v3_latin_mobile_FINETUNING_1/best_accuracy Global.save_inference_dir=./inference```

Une fois cette commande ex√©cut√©e, le mod√®le fut converti et stock√© dans le dossier inference. 
Le mod√®le est alors compos√© de trois nouveaux fichiers : inference.pdiparams, qui est le fichier de param√®tres du mod√®le, inference.pdiparams.info, qui est le fichier d'informations du mod√®le et inference.pdmodel, qui est le fichier de programme du mod√®le en lui-m√™me.
Afin de facilit√© l'utilisation du nouveau mod√®le, je cr√©e un dossier ocr_model et j'ai ajout√© dans ce dossier les trois fichiers d'inf√©rence convertis, le fichier de configuration du mod√®le en .yml ainsi que le dictionnaire utilis√© en .txt.

INSERER ICI UNE IMAGE DU DOSSIER OCR_MODEL

### 3.1.9 Le stockage du mod√®le

Afin de pouvoir utiliser ce mod√®le dans ce projet et dans d'autres √† venir, j'ai d√©cid√© de le stocker sur le Hub Hugging Face. Pour cela, j'ai cr√©√© un nouveau repository sur mon compte Hugging Face et j'ai ajout√© les fichiers du mod√®le dans ce repository.
Le mod√®le est donc maintenant disponible √† l'adresse suivante : https://huggingface.co/PaulineSanchez/PaddleOCR_ft

INSERER ICI UNE IMAGE DU HUB HUGGING FACE AVEC LE MODELE DANS LE REPOSITORY

# 3.2 Les mod√®les de traduction

### 3.2.1 Le choix des mod√®les

Comme pour l'OCR, une multitude de mod√®les de traduction existe. Depuis quelques ann√©es, les entreprises utilisent de plus en plus les mod√®les de traduction neuronaux. Ces mod√®les sont des mod√®les de traduction automatique qui utilisent des r√©seaux de neurones artificiels. Ils sont capables de traduire des textes d'une langue √† une autre en s'appuyant d'avantage sur le contexte. Parmi ces architectures, on retrouve notamment les RNN et les LSTM.
Les r√©seaux de neurones r√©currents (RNN) sont des mod√®les con√ßus pour traiter des s√©quences de texte en entr√©e et/ou g√©n√©rer des s√©quences de texte en sortie. Leur caract√©ristique principale est leur capacit√© √† conserver une forme de m√©moire gr√¢ce √† des boucles r√©currentes au sein de leurs couches cach√©es. Cela permet √† l'information contextuelle de circuler √† travers le r√©seau, de sorte que les sorties pr√©c√©dentes puissent influencer les calculs effectu√©s √† chaque √©tape temporelle.

Cette capacit√© √† m√©moriser les informations contextuelles est similaire √† notre fa√ßon de lire. Lorsque nous lisons, nous retenons les √©l√©ments importants des mots et des phrases pr√©c√©dentes pour les utiliser comme r√©f√©rence et comprendre le sens des nouveaux mots et des nouvelles phrases.
Les LSTM (Long Short-Term Memory) sont une variante des RNN. En effet, bien que les RNN soient capables de conserver des informations sur de courtes p√©riodes de temps, ils ont du mal √† conserver des informations sur de longues p√©riodes de temps. C'est ce que l'on appelle le probl√®me de la disparition du gradient. Les LSTM peuvent donc √™tre consid√©r√©s comme des RNN am√©lior√©s. Les LSTM sont compos√©s de trois portes : la porte d'oubli, la porte d'entr√©e et la porte de sortie. La porte d'oubli permet de d√©cider quelles informations doivent √™tre oubli√©es ou conserv√©es. La porte d'entr√©e permet de d√©cider quelles nouvelles informations doivent √™tre stock√©es dans la cellule d'√©tat. Enfin, la porte de sortie permet de d√©cider quelles informations doivent √™tre renvoy√©es en sortie. Ainsi, gr√¢ce √† leur structure sp√©cifique, les LSTM sont capables de capturer et de retenir des informations pertinentes sur une longue s√©quence, permettant ainsi de conserver la relation entre les mots √† travers diff√©rentes parties de la s√©quence. Ils sont donc particuli√®rement adapt√©s √† la traduction de textes longs. 

Cependant, une r√©volution majeure dans le domaine de la traduction automatique est survenue avec l'av√®nement des Transformers. Contrairement aux LSTM, qui sont bas√©s sur des architectures r√©currentes, les Transformers utilisent une approche bas√©e sur l'attention pour capturer les relations entre les mots. Cette architecture r√©volutionnaire a permis de surmonter les limitations des LSTM en termes de gestion des d√©pendances √† long terme et de parall√©lisme. Les Transformers se sont impos√©s comme une avanc√©e majeure dans la traduction automatique en raison de leur capacit√© √† traiter efficacement les s√©quences de texte et √† capturer les d√©pendances contextuelles sur de plus longues distances. Gr√¢ce √† leur m√©canisme d'attention, les Transformers peuvent attribuer des poids diff√©rents aux mots en fonction de leur importance, ce qui leur permet de comprendre le contexte global de la phrase et d'effectuer des traductions plus pr√©cises et coh√©rentes.

C'est pourquoi, j'ai donc d√©cid√© d'utiliser des mod√®les de traduction neuronaux bas√©s sur les Transformers. Afin de choisir quels mod√®les j'allais utiliser et finetuner, j'ai √©t√© lire le cours sur la traduction de Hugging Face. Plusieurs mod√®les √©taient propos√©s, comme par exemple T5, MarianMT, Bart...
T5 (Text-To-Text Transfer Transformer) est un mod√®le de traitement du langage naturel d√©velopp√© par Google. Il s'agit d'un mod√®le bas√© sur les Transformers qui peut √™tre utilis√© pour diverses t√¢ches, dont la traduction. 
Bart (Bidirectional and AutoRegressive Transformers") est un mod√®le d√©velopp√© par Facebook AI. Il est lui aussi bas√© sur les Transformers et a √©t√© sp√©cifiquement con√ßu pour la g√©n√©ration de texte. 
MarianMT est un projet open-source qui se concentre sur la traduction automatique neuronale. Il s'agit d'une impl√©mentation bas√©es sur les Transformers. MarianMT utilise le m√™me mod√®le de base que Bart avec quelques modifications.

Souhaitant utiliser des mod√®les sp√©cialis√©s dans les t√¢ches de traduction neuronaux bas√©s sur les Transformers, j'ai donc d√©cid√© de finetuner des mod√®les bas√©s sur MarianMT.

### 3.2.2 Les donn√©es n√©cessaires √† l'entra√Ænement des mod√®les

Souhait faire de la traduction de l'anglais vers le fran√ßais et du fran√ßais vers l'anglais, j'ai choisi de finetuner les deux mod√®les suivants :
`Helsinki-NLP/opus-mt-fr-en` pour la traduction du fran√ßais vers l'anglais, et `Helsinki-NLP/opus-mt-en-fr` pour la traduction de l'anglais vers le fran√ßais.
Pour finetuner ces mod√®les, j'allais avoir besoin d'un dataset contenant des phrases en anglais et leur traduction en fran√ßais. Afin d'obtenir de bonnes performances, il fallait que ce dataset soit compos√© de phrases plus ou moins longues et de phrases plus ou moins complexes. De plus, le but de ce finetuning √©tait pour moi d'obtenir des mod√®les sp√©cialis√©s dans la th√©matique de la cuisine. Il fallait donc que le dataset contienne des phrases en lien avec la cuisine. Ces phrases devaient contenir des verbes d'action propres √† la cuisine et √† la r√©alisation de recettes, des noms d'ingr√©dients, des noms de plats, des noms d'ustensiles... Par ailleurs, plus le dataset allait √™tre volumineux, plus les performances des mod√®les allaient √™tre bonnes.


### 3.2.3 La cr√©ation du dataset

De nombreux datasets de traduction sont disponibles sur les sites de Kaggle ou m√™me d'Hugging Face. Cependant, aucun ne correspondait √† mon th√®me, qui √©tait la cuisine, dans les deux langues choisies, le fran√ßais et l'anglais. Comme pour le finetuning d'OCR, j'ai donc d√©cid√© de cr√©er mon propre dataset. Pour cela, j'ai utilis√© plusieurs sites de cuisine : marmiton.org, allrecipes.com, simplyrecipes.com, mercotte.fr et quelques autres. 
A chaque fois, je selectionnais une ou plusieurs phrases qui me semblaient int√©ressantes, je les copiais, les collais dans la zone de texte du site deepl.com et je les traduisais dans la langue cible. Ayant des connaissances dans la traduction en anglais, je pouvais dans un second temps faire les modifictions n√©cessaires afin d'obtenir une traduction de qualit√©. Il ne me restait plus qu'√† copier les phrases en anglais et leur traduction en fran√ßais, et inversement, dans mon fichier CSV.
J'ai utilis√© deepl.com car il s'agit d'un site de traduction qui utilise des mod√®les neuronaux et qui est donc plus performant que Google Traduction par exemple. 

Afin de pouvoir finetuner mes mod√®les, le moyen le plus efficace √©tait de cr√©er des datasets avec la librairie Datasets de Hugging Face. Cette librairie permet de cr√©er des datasets √† partir de fichiers JSON, CSV, TXT, XML, etc. 
Il fallait donc dans un premier temps que je cr√©e un fichier CSV avec les donn√©es sur lesquelles je souhaitais finetuner mon mod√®le.
Le CSV devait avoir une colonne `id` qui s'incr√©mente √† chaque nouvelle entr√©e et une colonne `translation` qui contient la phrase en anglais et sa traduction en fran√ßais.
J'ai donc cr√©e un CSV avec ces caract√©ristiques et il ne me restait plus qu'√† le remplir avec les donn√©es que je souhaitais. 
Au final, mon fichier CSV √©tait constitu√© de 399 entr√©es. 

### 3.2.4 Le pr√©traitement des donn√©es

Avant de transformer mon fichier CSV en Dataset Hugging Face, je n'ai pas fait de data augmentation. J'ai estim√© que les donn√©es que j'avais s√©lectionn√©es √©taient suffisantes pour finetuner mes mod√®les. 
J'ai donc utilis√© la librairie Datasets de Hugging Face pour transformer mon fichier CSV en Dataset Hugging Face. Lors de l'ex√©cution du script, j'ai pris soin de pr√©ciser que la colonne `translation` √©tait une colonne de type `Translation` afin que le Dataset Hugging Face soit bien un dataset de traduction.

INSERER ICI LE CODE POUR TRANSFORMER LE CSV EN DATASET HUGGING FACE

J'ai ensuite envoy√© mon dataset sur le Hub de Hugging Face o√π il a √©t√© stock√© √† l'adresse suivante `https://huggingface.co/datasets/PaulineSanchez/recipes_translation_4_helsinki_4.0`.

Apr√®s cela, et apr√®s avoir v√©rifi√© que mon dataset √©tait correct et bien disponible sur le Hub de Hugging Face, je l'ai t√©l√©charg√©, et toujours avec la librairie Datasets de Hugging Face, j'ai divis√© mon dataset en deux datasets : un dataset d'entra√Ænement et un dataset de validation. J'ai choisi de diviser mon dataset de la mani√®re suivante, 80% des donn√©es pour l'entra√Ænement et 20% pour la validation. Cela donnait un dataset d'entra√Ænement de 319 entr√©es et un dataset de validation de 80 entr√©es.

INSERER ICI LE CODE POUR DIVISER LE DATASET EN DATASET D'ENTRAINEMENT ET DATASET DE VALIDATION

### 3.2.5 Le stockage du dataset

Comme pour le dataset avant la s√©paration en jeu d'entra√Ænement et en jeu de validation, j'ai envoy√© mon dataset sur le Hub de Hugging Face gr√¢ce √† la fonction `push_to_hub` de Hugging Face. Mon dataset a √©t√© stock√© √† l'adresse suivante `https://huggingface.co/datasets/PaulineSanchez/recipes_translation_400`. Ce dataset en affich√© en mode public afin que tout le monde puisse y avoir acc√®s et l'utiliser.

INSERER ICI LE CODE POUR ENVOYER LE DATASET SUR LE HUB DE HUGGING FACE + FLOUTER LE TOKEN
INSERER ICI UNE CAPTURE D'ECRAN DU DATASET SUR LE HUB DE HUGGING FACE

### 3.2.6 L'entra√Ænement des mod√®les

Pour entra√Æner mes mod√®les, j'ai utilis√© un script que j'ai trouv√© sur le Github de Hugging Face, dans le repository `transformers/examples/pytorch/translation`. Ce script permet d'entra√Æner un mod√®le de traduction, selon si son architecure est bas√©e sur les transformers, √† partir d'un dataset Hugging Face ou de fichiers json/CSV. J'ai donc utilis√© ce script pour entra√Æner mes deux mod√®les de traduction.
Afin de pouvoir utilis√© ce script, j'ai d√ª cloner le repository `transformers` de Hugging Face sur mon ordinateur. 
Apr√®s cela, apr√®s m'√™tre d√©plac√© dans le dossier, j'ai lanc√© la commande suivante :

```$ python examples/pytorch/translation/run_translation.py     --model_name_or_path Helsinki-NLP/opus-mt-en-fr     --do_train     --do_eval     --source_lang en     --target_lang fr     --dataset_name PaulineSanchez/recipes_translation_400  --output_dir /home/pauline/Documents/PCO/Modele_Traduction/train_hf_new_batch_4_epoch_6     --per_device_train_batch_size=6     --per_device_eval_batch_size=6     --predict_with_generate --report_to wandb --num_train_epochs 6 ```

Cette commande permet de lancer l'entra√Ænement du mod√®le de traduction. Elle prend en param√®tre le nom du mod√®le √† utiliser, le code de la langue source, le code de la langue cible, le nom du dataset √† utiliser, le chemin vers le dossier o√π stocker les r√©sultats de l'entra√Ænement, la taille du batch d'entra√Ænement, la taille du batch de validation, le nom de l'outil utilis√© pour faire du monitoring et le nombre d'epochs √† effectuer. 

J'ai donc lanc√© cette commande plusieurs fois, en inversant les langues source et cible et en pr√©cisant le nom du mod√®le √† utiliser. J'ai √©galement chang√© le nom du dossier o√π stocker les r√©sultats de l'entra√Ænement afin de pouvoir les diff√©rencier.

### 3.2.7 Configuration des hyperparam√®tres des mod√®les

Lors des diff√©rents entra√Ænements que j'ai lanc√©, les hyperparam√®tres que j'ai modifi√©s sont les suivants :
- le batch size : j'ai fait des tests avec des batch allant de 2 √† 16
- le nombre d'epochs : j'ai fait des tests avec des epochs allant de 3 √† 75

En fonction des r√©sultats que j'obtenais et des courbes dessin√©es sur Weight and Biases, j'ai pu ajuster les hyperparam√®tres au fur et √† mesure afin d'obtenir les meilleurs mod√®les possibles.

AJOUTER CAPTURE D'ECRAN DES COURBES DE W&B

### 3.2.8 Comparaison des mod√®les et choix des meilleurs

La m√©trique utilis√©e par Hugging Face pour comparer les mod√®les de traduction est le score BLEU (bilingual evaluation understudy). Le BLEU score est une m√©trique qui permet de comparer la qualit√© d'une traduction automatique √† une ou plusieurs traductions de r√©f√©rence. Il √©value la similarit√© entre les n-grammes (s√©quences de n mots cons√©cutifs) pr√©sents dans la sortie g√©n√©r√©e et ceux pr√©sents dans les r√©f√©rences. Le score est compris entre 0 et 1. Plus le score est proche de 1, plus la traduction est de bonne qualit√©. Cependant, le score BLEU pr√©sente √©galement certaines limites. Par exemple, il ne tient pas compte de la s√©mantique ou de la coh√©rence globale du texte, ce qui signifie qu'un score √©lev√© ne garantit pas n√©cessairement une traduction ou un r√©sum√© de haute qualit√© sur tous les aspects.

Ainsi, afin de pouvoir comparer les diff√©rents mod√®les entra√Æn√©s, j'ai l√† aussi utilis√© Weight and Biases. 
J'ai class√© mes mod√®les selon leur score BLEU et j'ai pu ainsi d√©terminer les meilleurs mod√®les.

INSERER ICI CAPTURE D'ECRAN DES MODELES ET DE LEUR SCORE BLEU

Ainsi, pour le mod√®le de traduction anglais-fran√ßais, le meilleur mod√®le est celui entra√Æn√© avec un batch size de 8 et 3 epochs. Pour le mod√®le de traduction fran√ßais-anglais, le meilleur mod√®le est celui entra√Æn√© avec un batch size de 12 et 4 epochs.

### 3.2.9 Le stockage des mod√®les

Une fois que j'ai d√©termin√© les meilleurs mod√®les, je les ai envoy√© sur le Hub de Hugging Face afin de pouvoir les utiliser par la suite. J'ai utilis√© la fonction `push_to_hub` de Hugging Face pour envoyer mes mod√®les sur le Hub. Il est a not√© que la fonction `push_to_hub` utilise Git et Git LFS pour envoyer les mod√®les sur le Hub. Ainsi, peu importe la taille du mod√®le, il sera envoy√© sur le Hub.
Mes mod√®les ont √©t√© stock√©s aux adresses suivantes `https://huggingface.co/PaulineSanchez/translation_for_recipes_en_fr` pour le mod√®le anglais vers fran√ßais et `https://huggingface.co/PaulineSanchez/translation_for_recipes_fr_en` pour le mod√®le fran√ßais vers anglais.
Ces mod√®les sont en mode public afin que tout le monde puisse y avoir acc√®s et les utiliser.

INSERER ICI LE CODE POUR ENVOYER LES MODELES SUR LE HUB DE HUGGING FACE + FLOUTER LE TOKEN

INSERER ICI UNE CAPTURE D'ECRAN DES MODELES SUR LE HUB DE HUGGING FACE

